<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>learning_log</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="../css/markdown.css">
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>
<a style="padding-right:20px;" href="./index.html">Home</a> <a style="padding-right:20px;" href="./curriculum.html">Curriculum</a> <a style="padding-right:20px;" href="./learning_log.html">Learning Log</a> <a style="padding-right:20px;" href="./bookshelf.html">Bookshelf</a>
</p>
<h2 id="learning-log">Learning Log</h2>
<h3 id="monday-24092018">Monday, 24/09/2018</h3>
<ul>
<li>I continued working on Chapter 5 of <a href="https://www.springer.com/gb/book/9781461471370?gclid=CjwKCAjwq57cBRBYEiwAdpx0vaZ-uhYESic2dZGkvVQSmZd6wznJ2LDQKAjkg81sJ_rp6s3mimNREhoCKwsQAvD_BwE">an Introduction to Statistical Learning</a>
<ul>
<li>I <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch5_statistical_learning/lab.ipynb">completed</a> the lab.</li>
<li>I <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch5_statistical_learning/exercises_conceptual.ipynb">completed</a> the conceptual questions.</li>
<li>I <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch5_statistical_learning/exercises_applied.ipynb">very nearly completed</a> the applied questions, just the last problem to finish off.</li>
</ul></li>
</ul>
<h3 id="friday-20092018">Friday, 20/09/2018</h3>
<ul>
<li>I <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch4_statistical_learning/exercises_applied.ipynb">completed</a> the applied excercises from Chapter 4 of <a href="https://www.springer.com/gb/book/9781461471370?gclid=CjwKCAjwq57cBRBYEiwAdpx0vaZ-uhYESic2dZGkvVQSmZd6wznJ2LDQKAjkg81sJ_rp6s3mimNREhoCKwsQAvD_BwE">an Introduction to Statistical Learning</a>.</li>
<li>I started working on Chapter 5: Resampling Methods
<ul>
<li>I worked through the <a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about">online lectures and questions</a>.</li>
<li>I read the corresponding chapter in <a href="https://www.springer.com/gb/book/9781461471370?gclid=CjwKCAjwq57cBRBYEiwAdpx0vaZ-uhYESic2dZGkvVQSmZd6wznJ2LDQKAjkg81sJ_rp6s3mimNREhoCKwsQAvD_BwE">an Introduction to Statistical Learning</a>.</li>
</ul></li>
</ul>
<h3 id="thursday-19092018">Thursday, 19/09/2018</h3>
<ul>
<li>I <a href="http://localhost:8888/notebooks/Desktop/mlsabbatical/notebooks/statistical_learning/ch4_statistical_learning/lab.ipynb">completed</a> the Chapter 4 Lab.</li>
<li>I <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch4_statistical_learning/exercises_applied.ipynb">very nearly completed</a> the applied excercises from Chapter 4 of <a href="https://www.springer.com/gb/book/9781461471370?gclid=CjwKCAjwq57cBRBYEiwAdpx0vaZ-uhYESic2dZGkvVQSmZd6wznJ2LDQKAjkg81sJ_rp6s3mimNREhoCKwsQAvD_BwE">an Introduction to Statistical Learning</a>, just the last problem to do.</li>
</ul>
<h3 id="wednesday-18092018">Wednesday, 18/09/2018</h3>
<ul>
<li>I <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch4_statistical_learning/exercises_conceptual.ipynb">completed</a> the conceptual excercises from Chapter 4 of <a href="https://www.springer.com/gb/book/9781461471370?gclid=CjwKCAjwq57cBRBYEiwAdpx0vaZ-uhYESic2dZGkvVQSmZd6wznJ2LDQKAjkg81sJ_rp6s3mimNREhoCKwsQAvD_BwE">an Introduction to Statistical Learning</a>.</li>
<li>I <a href="http://localhost:8888/notebooks/Desktop/mlsabbatical/notebooks/statistical_learning/ch4_statistical_learning/lab.ipynb">started</a> the Chapter 4 Lab.</li>
</ul>
<h3 id="tuesday-18092018">Tuesday, 18/09/2018</h3>
<p>Back to the books:</p>
<ul>
<li>I read Chapter 4 of <a href="https://www.springer.com/gb/book/9781461471370?gclid=CjwKCAjwq57cBRBYEiwAdpx0vaZ-uhYESic2dZGkvVQSmZd6wznJ2LDQKAjkg81sJ_rp6s3mimNREhoCKwsQAvD_BwE">an Introduction to Statistical Learning</a>.</li>
<li>I completed the lectures covering Chapter 4 from <a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about">Hastie and Tibshirani’s Statistical Learning course</a>.</li>
<li>I <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch4_statistical_learning/exercises_conceptual.ipynb">started the conceptual excercises</a> from Chapter 4.</li>
</ul>
<h3 id="monday-17092018">Monday, 17/09/2018</h3>
<p>Back from holiday! I spent the day playing around with some applied project ideas that I hope to share once I’ve articulated them better. I intend to spent one day a week on the applied track from now on.</p>
<h3 id="wednesday-4092018">Wednesday, 4/09/2018</h3>
<p>A half day, I <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch3_statistical_learning/exercises_applied.ipynb">finished</a> the applied exercises from chapter 3 of <a href="https://www.springer.com/gb/book/9781461471370?gclid=CjwKCAjwq57cBRBYEiwAdpx0vaZ-uhYESic2dZGkvVQSmZd6wznJ2LDQKAjkg81sJ_rp6s3mimNREhoCKwsQAvD_BwE">an Introduction to Statistical Learning</a>. Now I’m off to Bruges for for the rest of the week for my birthday, then I’m away next week in Greece for a holiday.</p>
<h3 id="tuesday-4092018">Tuesday, 4/09/2018</h3>
<p>I <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch3_statistical_learning/exercises_applied.ipynb">continued</a> the applied exercises from chapter 3 of <a href="https://www.springer.com/gb/book/9781461471370?gclid=CjwKCAjwq57cBRBYEiwAdpx0vaZ-uhYESic2dZGkvVQSmZd6wznJ2LDQKAjkg81sJ_rp6s3mimNREhoCKwsQAvD_BwE">an Introduction to Statistical Learning</a>. Which lead me to read <a href="https://www.statsmodels.org/dev/examples/notebooks/generated/regression_plots.html">this page</a> from the statsmodels docs about diagnostic regression plots. I also played around with <a href="https://patsy.readthedocs.io/en/latest/">patsy</a> which I discovered via the <a href="https://www.statsmodels.org/dev/example_formulas.html">statsmodels formula API</a> and skimmed the <a href="https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html">scipy.stats</a> documentation.</p>
<h3 id="monday-3092018">Monday, 3/09/2018</h3>
<p>I <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch3_statistical_learning/exercises_applied.ipynb">started</a> the applied exercises from chapter 3 of <a href="https://www.springer.com/gb/book/9781461471370?gclid=CjwKCAjwq57cBRBYEiwAdpx0vaZ-uhYESic2dZGkvVQSmZd6wznJ2LDQKAjkg81sJ_rp6s3mimNREhoCKwsQAvD_BwE">an Introduction to Statistical Learning</a> which I am porting from R to Python. I reviewed some of the material from the chapter and reached out to wikipedia and <a href="https://www.amazon.co.uk/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E">The Elements of Statistical Learning</a> for more detail. <a href="https://medium.com/@emredjan/emulating-r-regression-plots-in-python-43741952c034">This article</a> was useful in reproducing the diagnostic regression plots that come for free with R.</p>
<h3 id="friday-31082018">Friday, 31/08/2018</h3>
<p>I completed reading chapter 3 from <a href="https://www.springer.com/gb/book/9781461471370?gclid=CjwKCAjwq57cBRBYEiwAdpx0vaZ-uhYESic2dZGkvVQSmZd6wznJ2LDQKAjkg81sJ_rp6s3mimNREhoCKwsQAvD_BwE">An Introduction to Statistical Learning</a> and <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch3_statistical_learning/exercises_conceptual.ipynb">did the conceptual excercises</a>. I reviewed some of the material from the chapter and reached out to wikipedia and <a href="https://www.amazon.co.uk/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E">The Elements of Statistical Learning</a> for more detail.</p>
<h3 id="thursday-30082018">Thursday, 30/08/2018</h3>
<p>I completed the Ch3 lectures and questions from <a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about">Hastie and Tibshirani’s Statistical Learning course</a>. I started reading the associated chapter from <a href="https://www.springer.com/gb/book/9781461471370?gclid=CjwKCAjwq57cBRBYEiwAdpx0vaZ-uhYESic2dZGkvVQSmZd6wznJ2LDQKAjkg81sJ_rp6s3mimNREhoCKwsQAvD_BwE">An Introduction to Statistical Learning</a>.</p>
<h3 id="wednesday-29082018">Wednesday, 29/08/2018</h3>
<p>I completed the Ch1 and Ch2 lectures and questions from <a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about">Hastie and Tibshirani’s Statistical Learning course</a>. I read the associated chapters from <a href="https://www.springer.com/gb/book/9781461471370?gclid=CjwKCAjwq57cBRBYEiwAdpx0vaZ-uhYESic2dZGkvVQSmZd6wznJ2LDQKAjkg81sJ_rp6s3mimNREhoCKwsQAvD_BwE">An Introduction to Statistical Learning</a> and <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch2_statistical_learning/exercises.ipynb">did the exercies</a>.</p>
<h3 id="tuesday-28082018">Tuesday, 28/08/2018</h3>
<p>I met with my study buddy to discuss our experience of the <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a> competition, work thorough some sticking points and discuss our next steps. We resolved to complete <a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about">Hastie and Tibshirani’s Statistical Learning course</a>.</p>
<h3 id="friday-24082018">Friday, 24/08/2018</h3>
<ul>
<li><p>I completed modules 5.1 and 5.2 (<a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">cross-validation</a>) from <a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about">Hastie and Tibshirani’s Statistical Learning course</a>. I cross referenced with the corresponding sections of <a href="https://www.amazon.co.uk/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E">The Elements of Statistical Learning</a>.</p></li>
<li><p>I completed modules 6.6, 6.7 and 6.8 (shrinkage methods, ridge and lasso regression, finding lambda) from <a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about">Hastie and Tibshirani’s Statistical Learning course</a>. I cross referenced with the corresponding sections of <a href="https://www.amazon.co.uk/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E">The Elements of Statistical Learning</a>.</p></li>
<li><p>I checked out the the scikit-learn <a href="http://scikit-learn.org/0.16/modules/cross_validation.html#cross-validation">cross validation</a>, <a href="http://scikit-learn.org/stable/modules/model_evaluation.html">model evaluation</a> and <a href="http://scikit-learn.org/stable/modules/pipeline.html">pipeline</a> docs and <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/house_prices/house_prices.ipynb">used them</a> in my <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a> notebook. Using lasso regression yielded a competition score in the middle of the table.</p></li>
</ul>
<h3 id="thursday-23082018">Thursday, 23/08/2018</h3>
<ul>
<li><p>I <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/house_prices/house_prices.ipynb">worked on</a> on the kaggle <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a> competition.</p></li>
<li><p>I read <em>Chapter 10: Predicting Continuous Target Variables with Regression Analysis</em> of <a href="https://books.google.co.uk/books/about/Python_Machine_Learning.html?id=HuxuawEACAAJ&amp;source=kp_book_description&amp;redir_esc=y">Python Machine Learning</a></p></li>
</ul>
<h3 id="wednesday-22082018">Wednesday, 22/08/2018</h3>
<ul>
<li>I <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/house_prices/house_prices.ipynb">started working</a> on the kaggle <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a> competition, by the end of the day I’d fought my way out of the bottom quartile.</li>
<li>I dipped in to a few of the <a href="http://pandas.pydata.org/pandas-docs/stable/">padas guides</a> as I went.</li>
<li>I watched <a href="https://www.youtube.com/watch?v=sLoap0zH4qg">Manipulating and analysing multi-dimensional data with Pandas</a>.</li>
</ul>
<h3 id="tuesday-21082018">Tuesday, 21/08/2018</h3>
<p>I met up with my study buddy, as we’d agreed last week.</p>
<ul>
<li><p>We reviewed some data sets, we decided to spent one week working on Kaggle’s <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a> competition.</p></li>
<li><p>We spent more time comparing notes from <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a> and helped eachother through a few sticking points.</p></li>
<li><p>We pontificated on various topics including: death, software and statistics.</p></li>
</ul>
<p>I read <a href="http://karpathy.github.io/2016/05/31/rl/">Andrej Karpathy’s deep reinforcement learning blog post</a>. I made a mental note to spend a couple of weekends sometime getting an agent running in an <a href="https://gym.openai.com/envs/#robotics">OpenAI Gym environment</a>.</p>
<h3 id="monday-20082018">Monday, 20/08/2018</h3>
<p>I had a look online for a dataset that would be suitable for the week. I looked at:</p>
<ul>
<li><a href="https://data.london.gov.uk/">London Data Store</a></li>
<li><a href="http://openmappingdata.lambeth.gov.uk">Lambeth Open Data</a></li>
<li><a href="https://www.herc.ox.ac.uk/downloads/health_datasets/browse-data-sets/london-public-health-observatory-lpho-diseases-datasets">London Public Health Observatory</a></li>
<li><a href="http://landregistry.data.gov.uk/app/ppd/search">Land Registry</a></li>
<li><a href="https://aws.amazon.com/opendata/">AWS Open Data</a></li>
<li><a href="https://kaggle.com/">Kaggle</a></li>
</ul>
<p>I took a closer look at <a href="http://jupyter.org/">Jupyter notebooks</a>, which I’ve been using a fair amount:</p>
<ul>
<li>I read the <a href="https://jupyter.readthedocs.io/en/latest/architecture/how_jupyter_ipython_work.html">architecture doc</a> and grokked <a href="https://jupyter.readthedocs.io/en/latest/architecture/visual_overview.html">how it fits in</a> with the other open source tools under the Jupyter umbrella.</li>
<li>I checked out some services in the sphere including <a href="https://mybinder.org/">binder</a>, <a href="https://www.kaggle.com/">kaggle</a> and <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html">AWS sagemaker</a>.</li>
</ul>
<p>I signed up for and poked around on <a href="https://www.kaggle.com/">kaggle</a>:</p>
<ul>
<li>Forked some kernels and ran them.</li>
<li>Entered the <a href="https://www.kaggle.com/c/titanic">Titanic: Machine Learning from Disaster</a> competition and <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/titanic/titanic.ipynb">submitted a trivial model</a> to get a feel for kaggle and competitions.</li>
</ul>
<p>I <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/playground/playground.ipynb">played with</a> some python libraries:</p>
<ul>
<li>I read the <a href="http://pandas.pydata.org/pandas-docs/stable/10min.html">pandas intro</a> and got familiar with <a href="http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dsintro">Series, DataFrame</a> and a handful of operations like <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html">groupby</a> and <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html">crosstab</a>.</li>
<li>I became aware of and got familiar with <a href="https://seaborn.pydata.org/introduction.html#introduction">seaborn</a>.</li>
</ul>
<h3 id="friday-17082018">Friday, 17/08/2018</h3>
<p>I met up with my study buddy.</p>
<ul>
<li>We compared notes on some of the sticking points we had from <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>.</li>
<li>We decided to do spend next week doing something applied. We’re going to meet up on Tuesday to kick off a week long sprint, in which we will select a dataset, perform some exploratory data analysis and make some predictions.</li>
<li>We pontificated on the future of the software industry.</li>
</ul>
<h3 id="thursday-16082018">Thursday, 16/08/2018</h3>
<p>I finished <a href="https://github.com/coxy1989/mlsabbatical/blob/master/homework/bloomberg_lec_check_1.md">my work</a> on the <em>Statistical Learning Theory</em> section of the <a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/1-Lec-Check.pdf">Bloomberg Concept Check 1</a>. I faired well on the subjects I’ve seen recently: like <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/bloomberg/bloomberg_lec_check_1.py">fitting linear and quadratic functions to data using the normal equation</a>. I faired less well, though not catastrophically, on the probabilty material, which I’ve yet to review methodically as part of this sabattical - I suspect I may need to do this - one to discuss with my study buddy next time we meet.</p>
<h3 id="wednesday-15082018">Wednesday, 15/08/2018</h3>
<ul>
<li><p>I did a unit of work in the <em>practice track</em> - <a href="https://yutsumura.com/if-the-nullity-of-a-linear-transformation-is-zero-then-linearly-independent-vectors-are-mapped-to-linearly-independent-vectors/">proving this</a>, I watched a couple of short Pavel Grinfeld lectures on the null space which enhanced my intution.</p></li>
<li><p>I started working on the <em>Statistical Learning Theory</em> section of the <a href="https://davidrosenberg.github.io/mlcourse/ConceptChecks/1-Lec-Check.pdf">Bloomberg Concept Check 1</a>.</p></li>
</ul>
<h3 id="tuesday-14082018">Tuesday, 14/08/2018</h3>
<p>I did some monumental admin which included booking: dentist, hygenist, haircut <strong><em>and</em></strong> car MOT. I went through the links, miscellaneous notes and TODOs I had left over from the <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a> I finished yesterday and pruned/consolidated them. I modified the <a href="https://www.coxy1989.com/curriculum.html">curriculum</a> to include a *“Practice track: A”little and often&quot; track; small programming exercises and mathematical problems. Intended to keep maintain and enhance practical skills.“*. I created the track to try to retain and enhance what I’ve learned so far, whilst still making headway in the foundational track.</p>
<p>I ordered another book - <a href="https://www.amazon.co.uk/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a>. Which is a referece text for the Bloomberg course.</p>
<p>I attended Lecture 2 from <a href="https://bloomberg.github.io/foml/#home">Bloomberg’s Foundations of Machine Learning</a>, which was a Case Study: We were asked to frame the problem of customer churn for a mobile network operator as a machine learning problem: predict when a user will churn. Again, the students made this an entertaining and informative session, some suggestions included: a probability distribution over the days in the future that a user may churn, a binary classification of churn/no-churn in some specified window in the future, and ‘number of days’ until churn prediction. The objective of this activity was to demonstrate that mapping the choice of outcome measure when approaching business problems is often non-trivial.</p>
<p>I attended Lecture 3 from <a href="https://bloomberg.github.io/foml/#home">Bloomberg’s Foundations of Machine Learning</a>, which was an introduction to Statistical Learning Theory, topics included:</p>
<ul>
<li>The definition of <em>input</em>, <em>action</em>, <em>output</em> spaces and the definition of <em>decision functions</em> and <em>loss functions</em> in terms of these spaces.</li>
<li>The assumptions made when analysing a problem using the Statistical Learning Theory framework.</li>
<li>The definition of <em>risk</em>, <em>bayes decision function</em>, <em>empirical risk</em>, <em>empirical risk minimizer</em>, <em>constrained empirical risk minimiser</em> and the <em>hypothesis space</em>.</li>
<li>Linear regression and multiclass classification from the perspective of Statistical Learning Theory.</li>
</ul>
<p>A fair amount of this language was new to me, I took the opportunity to read the introduction to <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning</a> and the <a href="https://en.wikipedia.org/wiki/Statistical_learning_theory">Statistical Learning Theory Wikipedia page</a> before writing up my notes from today’s lectures.</p>
<p>I’ll take down the concept check and homework problems tomorrow.</p>
<h3 id="monday-13082018">Monday, 13/08/2018</h3>
<p>I worked through the Principal Components Analysis proof on pages 392-3 of <a href="https://www.amazon.co.uk/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020">Murphy’s MLPP</a>. Feeling confident following this proof was a satisfying capstone to <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a> as it requied the application of much of the knowledge I’ve acquired over the past few weeks. The proof begins by constructing an expression for the projection error and shows that that it is minimized when the projection onto the subspace is orthonormal, before demonstrating that minimising the projection error is equivalent to maximising the variance of the projected data. This allows one to write an expression for the variance of the projected data in terms of the covariance matrix of the high dimensional data which we can then maximize in a constrained optimization - making use of a Lagrange multiplier. This maximization yields an expression for the variance of the projected data that can be recognised as an eigen problem - we arrive at an expression identifying the vector in the direction of maximal variance as an eigenvector of the covariance matrix with the largest eigenvalue.</p>
<p>I wrote up my notes from the last module of <a href="https://www.coursera.org/learn/pca-machine-learning">Mathematics for Machine Learning: PCA</a> which I finished on saturday morning and incorporated the proof from <a href="https://www.amazon.co.uk/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020">Murphy’s MLPP</a>.</p>
<p>Though I still have some loose ends to tie up with <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>, I permitted myself to watch the first lecture from <a href="https://bloomberg.github.io/foml/#home">Bloomberg’s Foundations of Machine Learning</a> which is the next item in the <a href="https://www.coxy1989.com/curriculum.html">curriculum</a>. The lecture was a gentle introduction to machine learning, though we are assured the learning curve is due to increase steeply. The content was a survey of the basics and the material was familiar from <a href="https://www.coursera.org/learn/machine-learning">Andrew Ng’s Machine Learning Course</a>: classification and regression, bias and overfitting, training/validation/test sets etc. The figures on polynomial curve fitting with a power series were familiar from <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a> and the introduction to <a href="https://www.amazon.co.uk/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738">Bishop’s PRML</a>. Most noteworthy were the good questions from the students in the lecture, I suspect some were software engineers as they shared my discomfort with the nature of deploying the non-deterministic artefacts of ML to production - the Q and A on this subject was interesting. The instructor mentioned the upcoming homeworks, I looked ahead and they look like they are pitched at the right level - I’m looking forward to getting to them and writing some code.</p>
<h3 id="friday-10082018">Friday, 10/08/2018</h3>
<p>I completed week 4 of <a href="https://www.coursera.org/learn/pca-machine-learning">Mathematics for Machine Learning: PCA</a>, topics included:</p>
<ul>
<li>The axiomatic definition of: Groups, Fields, Vector Spaces, Vector Subspaces and their orthogonal complement.</li>
<li>The objective of PCA; equivalence of maximising variance of projected data and minimizing projection error.</li>
<li>Deriving a proof for Principal Components Analysis.</li>
<li>Practical considerations when performing PCA on a data set.</li>
<li>Programming Exercise: Implementing PCA.</li>
</ul>
<p>This unit was the most detailed so far. I didn’t have time to write up my notes and I ended up finishing the programming exercise on Saturday morning. I’ll write up my notes on monday morning and that’ll conclude the <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a> series.</p>
<h3 id="thursday-9082018">Thursday, 9/08/2018</h3>
<p>I completed week 3 of <a href="https://www.coursera.org/learn/pca-machine-learning">Mathematics for Machine Learning: PCA</a>, topics included:</p>
<ul>
<li>Orthogonal projections using different inner products.</li>
<li>Orthogonal projections as reconstruction error in the context of dimensionality reduction.</li>
<li>Orthogonal projection with numpy using the <a href="http://scikit-learn.org/stable/datasets/olivetti_faces.html">Olivetti faces dataset</a> and the <a href="http://scikit-learn.org/stable/datasets/#boston-house-prices-dataset">Boston house prices dataset</a>.</li>
</ul>
<h3 id="wednesday-8082018">Wednesday, 8/08/2018</h3>
<p>I completed week 2 of <a href="https://www.coursera.org/learn/pca-machine-learning">Mathematics for Machine Learning: PCA</a>, topics included:</p>
<ul>
<li>The axiomatic definition of an inner product.</li>
<li>The dot product as an example of an inner product.</li>
<li>General definition of norms angles and orthogonality with respect to the inner product.</li>
<li>The inner product over a continuous domain; the inner product of a pair of functions as an integral and the inner product of a pair of random variables as their covariance.</li>
</ul>
<p>During the week’s programming exercise I took a detour to review <a href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html">broadcasting</a> with numpy and read the relevant chapter from the <a href="https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html">python data science handbook</a>. I had an understanding of broadcasting that served me well in performing binary operations between scalars and arrays, and between pairs of arrays. My intuition was however not robust enough to generalise well to broadcasting pairs of matrices - which requires an intuition fit for three dimensions, after some <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/playground/broadcasting.py">playing around</a> I grokked it.</p>
<h3 id="tuesday-7082018">Tuesday, 7/08/2018</h3>
<p>I completed week 1 of <a href="https://www.coursera.org/learn/pca-machine-learning">Mathematics for Machine Learning: PCA</a> which covered some elementary statistics material, topics included:</p>
<ul>
<li>Expected Value, Variance and Covariance</li>
<li>Effect of a linear transformation of the dataset on the moments of the distibution.</li>
<li>Statistical operations in numpy.</li>
</ul>
<p>I started week 2 of <a href="https://www.coursera.org/learn/pca-machine-learning">Mathematics for Machine Learning: PCA</a>, which began with a refresher of the dot product before moving on to the more general definition of an inner product.</p>
<h3 id="monday-6082018">Monday, 6/08/2018</h3>
<p>I completed week 6 of <a href="https://www.coursera.org/learn/multivariate-calculus-machine-learning">Mathematics for Machine Learning: Multivariate Calculus</a>, topics included:</p>
<ul>
<li>Regression as a minimisation of errors problem</li>
<li>Distinguish appropriate models for particular data sets</li>
<li>Fitting functions to data using gradient descent</li>
</ul>
<p>This concluded <a href="https://www.coursera.org/learn/multivariate-calculus-machine-learning">Mathematics for Machine Learning: Multivariate Calculus</a>. I did a quick review of the course.</p>
<h3 id="friday-3082018">Friday, 3/08/2018</h3>
<p>I completed week five of <a href="https://www.coursera.org/learn/multivariate-calculus-machine-learning">Mathematics for Machine Learning: Multivariate Calculus</a>. The week’s focus was numerical optimisation, topics covered:</p>
<ul>
<li>The Newton-Raphson method</li>
<li>Gradient Descent</li>
<li>Constrained Optimization: The method of Lagrange Multipliers</li>
</ul>
<h3 id="thursday-2082018">Thursday, 2/08/2018</h3>
<p>I completed the second half of week four of <a href="https://www.coursera.org/learn/multivariate-calculus-machine-learning">Mathematics for Machine Learning: Multivariate Calculus</a> and wrote up my notes for the week, new topics in the second half of the week were:</p>
<ul>
<li>Multivariate Taylor series</li>
<li>Linearisation</li>
</ul>
<p>I registered for <a href="https://www.meetup.com/London-Data-Science-Journal-Club/events/253318286/">a meetup</a> next month that my study buddy discovered; from the description: <em>“There is NO speaker at Journal Club. We split into small groups of 6 people and discuss the papers. For the first hour the groups are random to make sure everyone is on the same page. Afterwards we split into blog/paper/code groups to go deeper”</em>. Some swatting up required to avoid blushes here.</p>
<h3 id="wednesday-1082018">Wednesday, 1/08/2018</h3>
<p>I met up with my study buddy. We discussed how we were getting on with the curriculum, we are both having too much of a good time in the foundational track and have been neglecting the applied and interview training tracks. Fair enough, the interview training track is pretty dull and we agreed the applied track can wait until the <a href="https://www.coursera.org/specializations/mathematics-machine-learning"><em>Mathematics for Machine learning</em></a> unit is wrapped up - which it should be within the next couple of weeks.</p>
<p>I completed the first half of week four of <a href="https://www.coursera.org/learn/multivariate-calculus-machine-learning">Mathematics for Machine Learning: Multivariate Calculus</a>, topics covered:</p>
<ul>
<li>Building approximate functions</li>
<li>Maclaurin series</li>
<li>Taylor series</li>
</ul>
<h3 id="tuesday-31072018">Tuesday, 31/07/2018</h3>
<p>I worked through weeks two and three of <a href="https://www.coursera.org/learn/multivariate-calculus-machine-learning">Mathematics for Machine Learning: Multivariate Calculus</a>, topics covered:</p>
<ul>
<li>Partial differentiation</li>
<li>The Jacobian matrix</li>
<li>The Hessian matrix</li>
<li>The multivariate chain rule</li>
<li>Applying the multivariate chain rule to train a neural network</li>
</ul>
<h3 id="monday-30072018">Monday, 30/07/2018</h3>
<p>I reviewed and consolidated my notes from <a href="https://www.coursera.org/learn/linear-algebra-machine-learning">Mathematics for Machine Learning: Linear Algebra</a> before moving onto the next course in the specialisation. I completed week one of <a href="https://www.coursera.org/learn/multivariate-calculus-machine-learning">Mathematics for Machine Learning: Multivariate Calculus</a>, which was a univariate differential calculus review covering:</p>
<ul>
<li>Definition of the derivative</li>
<li>The sum, power, product and chain rules</li>
<li>The derivatives of 1/x, e^x and trig functions.</li>
</ul>
<h3 id="friday-27072018">Friday, 27/07/2018</h3>
<p>I completed the assignments for week 5 of the <a href="https://www.coursera.org/learn/linear-algebra-machine-learning">Mathematics for Machine Learning: Linear Algebra</a>, which included a quiz on diagonalization and an implementation of power iteration. This concluded the course, I had a flick back through the course. I will do a review of the material on Monday before moving onto the next course in the series.</p>
<h3 id="thursday-26072018">Thursday, 26/07/2018</h3>
<p>I completed the assignments for week 4 and worked through week 5 of the <a href="https://www.coursera.org/learn/linear-algebra-machine-learning">Mathematics for Machine Learning: Linear Algebra</a>. Week 5’s topic is eigenvectors/values.</p>
<p>I completed the timetabling excercise on InterviewCake.</p>
<h3 id="wednesday-25072018">Wednesday, 25/07/2018</h3>
<p>I worked through week 4 of <a href="https://www.coursera.org/learn/linear-algebra-machine-learning">Mathematics for Machine Learning: Linear Algebra</a>, which continued yesterday’s linear algebra review. Topics included:</p>
<ul>
<li>Einstein summation notation</li>
<li>Transformations in a changed basis</li>
<li>Properties of orthogonal matrices</li>
<li>The Gram-Schmidt process</li>
</ul>
<p>I started my systematic transit through InterviewCake, I did the readings in the first two sections: “Algorithmic Thinking” and “Array and string manipulation“ which was a back to basics CS101 style intro to the rest of the material on the site.</p>
<p>I added some more thoughts to the <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notes/09-applied-track.md">applied track doc</a></p>
<h3 id="tuesday-24072018">Tuesday, 24/07/2018</h3>
<p>I completed the first three weeks of <a href="https://www.coursera.org/learn/linear-algebra-machine-learning">Mathematics for machine learning: Linear Algebra</a>. This was a nice back to basics linear algebra review, which I didn’t mind as it felt good to stake out some ground - topics included:</p>
<ul>
<li>Dot product</li>
<li>Scalar and vector projection</li>
<li>Changing basis</li>
<li>Linear Independence</li>
<li>Matrix transformations and their composition</li>
<li>Gaussian elimination</li>
<li>Matrix inverses</li>
<li>Determinants</li>
</ul>
<p>I signed up for and had a poke around on <a href="https://www.interviewcake.com/">interviewcake</a> to get a feel for it. I’ll start a more systematic transit through the material tomorrow.</p>
<p>I created <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notes/09-applied-track.md">a document</a> to track project ideas for the applied track.</p>
<h3 id="monday-23072018">Monday, 23/07/2018</h3>
<p>I met up with my study buddy, we compared notes and constructed <a href="https://www.coxy1989.com/curriculum.html">our curriculum</a>.</p>
<h3 id="friday-20072018">Friday, 20/07/2018</h3>
<p>I reviewed:</p>
<ul>
<li>Recommender systems / low rank matrix factorization</li>
<li>Batch / Mini Batch / Stocastic GD</li>
</ul>
<p>Which concluded my review of my notes from Andrew Ng’s ML course.</p>
<p>I reviewed the curricula from some masters courses and made notes <a href="https://github.com/coxy1989/mlsabbatical/tree/master/notes">here</a></p>
<h3 id="thursday-19072018">Thursday, 19/07/2018</h3>
<p>Review of <a href="https://www.coursera.org/learn/machine-learning">Andrew Ng’s Machine Learning</a> topics:</p>
<ul>
<li>K-Means</li>
<li>Principal Components Analysis</li>
<li>Anomaly Detection</li>
</ul>
<p>In the afternoon I continued working through chapter 3 of “Python Machine Learning”.</p>
<h3 id="wednesday-18072018">Wednesday, 18/07/2018</h3>
<p>Review of <a href="https://www.coursera.org/learn/machine-learning">Andrew Ng’s Machine Learning</a> topics:</p>
<ul>
<li>Neural Networks</li>
<li>Bias &amp; Variance, Precsion &amp; Recall</li>
<li>SVMs</li>
</ul>
<p>In the afternoon I started working through chapter 3 of “Python Machine Learning”.</p>
<p>I took a look at the UCL machine learning masters syllabus and made notes <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notes/02-ucl-masters-curriculum.md">here</a></p>
<h3 id="tuesday-17072018">Tuesday, 17/07/2018</h3>
<p>Revisited my notes from <a href="https://www.coursera.org/learn/machine-learning">Andrew Ng’s course</a> and cross referenced some topics with some more advanced resources. I found the book I was a little afraid of - <a href="https://www.amazon.co.uk/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738">Bishop’s PRML</a> is challenging but well written and accessible.</p>
<p>I reviewed:</p>
<ul>
<li>Linear Regression</li>
<li>Logistic Regression</li>
<li>Overfitting &amp; Regularisation</li>
</ul>
<h3 id="monday-16072018">Monday, 16/07/2018</h3>
<p>Morning working on my <a href="https://www.coxy1989.com/curriculum.html">curriculum</a>. Activities have included:</p>
<ul>
<li>Reviewing Karl Rosean’s <a href="http://karlrosaen.com/ml/learning-log/">learning log</a> (my notes <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notes/01-karl-rosaen-learning-log-notes.md">here</a>) and <a href="http://karlrosaen.com/ml/">resources</a></li>
<li>Scanning resources</li>
<li>Writing up curriculum page and this log</li>
</ul>
<p>I hope to get a first draft out today and solicit some feedback.</p>
<p>I met up with a colleague in the afternoon. He’s interested in pursuing a machine learning sabbatical of his own which is fantastic news. After talking for a few hours I’m convinced that I should take a slightly more measured approach to planning my curriculum. I’m going to take a step back and explore the space of possible curricula a little further before seeking wider feedback. I plan to systematicaly review curricula offered from masters programmes and look at the requirements on ML job listings. I’ll also continue to thumb through more resources and feel out what looks promising. I’m going to meet up with said colleague early next week, we intend compare notes on what to include in what we plan on learning, we’ve agreed it makes sense to share a set of ‘core modules’ but to have the freedom to also pursue ‘optional modules’ so that we’re not shackled to eachother and can still pursue interest.</p>
<p>I’m going to spend the rest of this week engaging in the following activities:</p>
<ul>
<li>Revisiting my notes from Andrew Ng’s Machine Learning course.</li>
<li>Cross referencing the topics is the Andrew Ng course with some of the resources I have discovered and feeling out which are suitable.</li>
<li>Reviewing curricula of masters programmes.</li>
<li>Exploring core and optional modules for inclusion in the curriculum.</li>
</ul>
<h3 id="sunday-15072018">Sunday, 15/07/2018</h3>
<p>A day off at Hampstead Heath. Felt out the Talking Machines Podcast, listening to the first 3 episodes.</p>
<p>In the first episode we were privy to a chat with: Yan LeCun, Yoshua Benugo and Geoff Hinton. I’m aware of LeCun having downloaded the MNIST dataset from <a href="http://yann.lecun.com/exdb/mnist/">his website</a> a while ago to do <a href="https://github.com/coxy1989/clj_mnist">clj_mnist</a>. I was also aware of Benugo as a co-author of the <a href="https://www.amazon.co.uk/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618">deep learning bible</a> and I’ve seen Hinton before in an interview with Andrew Ng in a coursera course - these guys are the Deep Learning Mafia. We also met Kevin Murphy who’s a head honcho at google and the author of <a href="https://www.amazon.co.uk/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020">Machine Learning a Probabalistic Perspective</a> which is fighting it out with <a href="https://www.amazon.co.uk/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738">Pattern Recognition and Machine Learning</a> to be the canonical advanced level machine learining reference - I’ll hope to graduate onto these books in the not too distant future (I bought PRML last week for the odd flick through, to gauge how deep the pool is).</p>
<p>In the second episode we met Ilya Sutskever. He’s an deep learning fan boy working at google. Amongst other things, he said he felt it was not well understood why it should be that gradient decent empirically appears to be an appropriate algorithm for finding good paramaters for deep NNs given the high-dimensional non-convex surface of the function they are tasked with optimizing. He linked this to the AI winter, saying that in the 80s/90s people had failed to train deep neural networks for other reasons (badly initialized weights in paticular) and incorrectly concluded that the optimisation problem posed by deep NNs was intractable. In the third episode the host dug out a relevant paper from Yoshua Bengio’s Lab entitled: “Identifying and attacking the saddle point problem in high-dimensional non-convex optimization”, the paper contains empirical and theoretical evidence that saddle points are more frequently the cause of slow training in large NNs than local minima, the paper also proposes some approaches for tackling this problem.</p>
<p>Summary</p>
<ul>
<li>A few people to follow on twitter to keep abreast of the moving frontier</li>
<li>A couple of earmarked textbooks</li>
<li>Interesting podcast, nice to get a historical context, nice technical level; not superficial but doesn’t require frequent pausing / taking of notes. Will continue to listen.</li>
</ul>
<h3 id="saturday-14072018">Saturday, 14/07/2018</h3>
<p>I spun up this website and began feeling out some resources that I may decide to include in the curriculum.</p>
</body>
</html>
