{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, model_selection\n",
    "sns.set(rc={'figure.figsize':(10,10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tin Kam Ho is credited with [the first description](https://web.archive.org/web/20160417030218/http://ect.bell-labs.com/who/tkh/publications/papers/odt.pdf) of random decision forests in 1995. Leo Brieman is credited with [the introduction of random forests proper]((https://link.springer.com/article/10.1023%2FA%3A1010933404324)) in 2001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1.51902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.605</td>\n",
       "      <td>8.375</td>\n",
       "      <td>93.9</td>\n",
       "      <td>2.1620</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>388.45</td>\n",
       "      <td>3.32</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.06899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.581</td>\n",
       "      <td>5.870</td>\n",
       "      <td>69.7</td>\n",
       "      <td>2.2577</td>\n",
       "      <td>2.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>389.15</td>\n",
       "      <td>14.37</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.20746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.093</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.8226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>318.43</td>\n",
       "      <td>29.68</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.11132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>83.5</td>\n",
       "      <td>2.1099</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.35</td>\n",
       "      <td>20.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>9.59571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693</td>\n",
       "      <td>6.404</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.6390</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>376.11</td>\n",
       "      <td>20.31</td>\n",
       "      <td>12.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM   ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
       "163  1.51902  0.0  19.58   1.0  0.605  8.375   93.9  2.1620   5.0  403.0   \n",
       "120  0.06899  0.0  25.65   0.0  0.581  5.870   69.7  2.2577   2.0  188.0   \n",
       "490  0.20746  0.0  27.74   0.0  0.609  5.093   98.0  1.8226   4.0  711.0   \n",
       "492  0.11132  0.0  27.74   0.0  0.609  5.983   83.5  2.1099   4.0  711.0   \n",
       "402  9.59571  0.0  18.10   0.0  0.693  6.404  100.0  1.6390  24.0  666.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  target  \n",
       "163     14.7  388.45   3.32    50.0  \n",
       "120     19.1  389.15  14.37    22.0  \n",
       "490     20.1  318.43  29.68     8.1  \n",
       "492     20.1  396.90  13.35    20.1  \n",
       "402     20.2  376.11  20.31    12.1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_data = datasets.load_boston()\n",
    "df_boston = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\n",
    "df_boston['target'] = pd.Series(boston_data.target)\n",
    "train, test = model_selection.train_test_split(df_boston, test_size=0.2)\n",
    "train_X = train.drop('target', axis=1).values\n",
    "train_y = train['target'].values\n",
    "test_X = test.drop('target', axis=1).values\n",
    "test_y = test['target'].values\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sum_split_variance(xs, y, v):\n",
    "    '''xs - 1D array of scalars\n",
    "        v - scalar to split on'''\n",
    "    left = y[xs <= v]\n",
    "    right = y[xs > v]\n",
    "    left_var = 0 if len(left) == 0 else ((left - left.mean()) ** 2).sum()\n",
    "    right_var = 0 if len(right) == 0 else ((right - right.mean()) ** 2).sum()\n",
    "    return  left_var + right_var\n",
    "\n",
    "def node(i, s, p, c, l, r):\n",
    "    return {'internal': i,\n",
    "            'split': s,\n",
    "            'p': p,\n",
    "            'c':c,\n",
    "            'l':l,\n",
    "            'r':r}\n",
    "\n",
    "def gen_tree(X, y, max_leaf_n, m):\n",
    "    if X.shape[0] <= max_leaf_n:\n",
    "        return node(False, None, None, y.mean(), None, None)\n",
    "    lowest_var, best_p_idx, best_split = sys.float_info.max, None, None\n",
    "    # Putting the 'random' in random forests..\n",
    "    ps = np.random.choice(np.arange(0,X.shape[1]), m, replace=False)\n",
    "    for p_idx in ps:\n",
    "        for n_idx in range(0, X.shape[0]):\n",
    "            split = X[n_idx][p_idx]\n",
    "            var = compute_sum_split_variance(X[:,p_idx], y, split)\n",
    "            if var < lowest_var:\n",
    "                lowest_var = var\n",
    "                best_p_idx = p_idx\n",
    "                best_split = split\n",
    "    left_idxs = X[:, best_p_idx] <= best_split\n",
    "    right_idxs = X[:, best_p_idx] > best_split\n",
    "    if len(y[left_idxs]) == 0 or len(y[right_idxs]) == 0:\n",
    "        # This happens when: yi = yi+1 ... = yn OR xi = xi+1 .... = xn\n",
    "        return node(False, None, None, y.mean(), None, None)\n",
    "    l = gen_tree(X[left_idxs], y[left_idxs], max_leaf_n, m)\n",
    "    r = gen_tree(X[right_idxs], y[right_idxs], max_leaf_n, m)\n",
    "    return node(True, best_split, best_p_idx, None, l, r)\n",
    "\n",
    "def predict(x, model):\n",
    "    if not model['internal']:\n",
    "        return model['c']\n",
    "    if x[model['p']] <= model['split']:\n",
    "        return predict(x, model['l'])\n",
    "    else:\n",
    "        return predict(x, model['r'])\n",
    "    \n",
    "def bagged_predict(x, trees):\n",
    "    return np.array([predict(x, t) for t in trees]).mean()\n",
    "    \n",
    "def bagged_trees(X, y, max_leaf_n, b, m):\n",
    "    n = X.shape[0]\n",
    "    trees = []\n",
    "    for _ in range(0,b):\n",
    "        b_sample = np.random.randint(0, n, n)\n",
    "        trees.append(gen_tree(X[b_sample], y[b_sample], max_leaf_n, m))\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: 2.7412998218931732\n"
     ]
    }
   ],
   "source": [
    "max_leaf_n = 2\n",
    "b = 200\n",
    "m = int(np.floor(np.sqrt(train_X.shape[1])))\n",
    "trees = bagged_trees(train_X, train_y, max_leaf_n, b, m)\n",
    "\n",
    "# evaluate test error\n",
    "preds = [bagged_predict(x, trees) for x in test_X]\n",
    "print('test error: {}'.format(np.sqrt(((test_y - preds) ** 2).sum() / test_y.shape[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
