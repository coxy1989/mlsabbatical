{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2, . . . , p predictors. Explain your answers:\n",
    "\n",
    "\n",
    "(a) Which of the three models with k predictors has the smallest training RSS?\n",
    "\n",
    "- The model obtained by best subset selection has training RSS equal to or smaller than the others. The best subset procedure considers all possible models for each number of predictors, forward stepwise, and backward stepwise do not. \n",
    "\n",
    "- For k=1, best subset and forward stepwise will always obtain the same model.\n",
    "\n",
    "- For k=p, best subset and backward stepwise will always obtain the same model.\n",
    "\n",
    "(b) Which of the three models with k predictors has the smallest test RSS?\n",
    "\n",
    "- The same reasoning above applies.\n",
    "\n",
    "(c)\n",
    "\n",
    "i. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.\n",
    "\n",
    "- True\n",
    "\n",
    "ii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1) variable model identified by backward stepwise selection.\n",
    "\n",
    "- False\n",
    "\n",
    "iii. The predictors in the k-variable model identified by back- ward stepwise are a subset of the predictors in the (k + 1)- variable model identified by forward stepwise selection.\n",
    "\n",
    "- False\n",
    "\n",
    "iv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1) variable model identified by backward stepwise selection.\n",
    "\n",
    "- False\n",
    "\n",
    "v. The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1) variable model identified by best subset selection.\n",
    "\n",
    "- False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer.\n",
    "\n",
    "(a) The lasso, relative to least squares, is:\n",
    "\n",
    "iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\n",
    "\n",
    "(b) Repeat (a) for ridge regression relative to least squares.\n",
    "\n",
    "iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\n",
    "\n",
    "(c) Repeat (a) for non-linear methods relative to least squares.\n",
    "\n",
    "ii. More flexible and hence will give improved prediction accu- racy when its increase in variance is less than its decrease in bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Suppose we estimate the regression coefficients in a linear regression model by minimizing: (LASSO optimisation objective)\n",
    "\n",
    "for a particular value of s. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer\n",
    "\n",
    "\n",
    "(a) As we increase s from 0, the training RSS will:\n",
    "\n",
    "iii. Steadily increase.\n",
    "\n",
    "- We will pay a price in increased bias and recieve no benefit from reduced variance.\n",
    "\n",
    "(b) Repeat (a) for test RSS.\n",
    "\n",
    "ii. Decrease initially, and then eventually start increasing in a U shape.\n",
    "\n",
    "- Initially, the increased bias of the model is offset by an even larger reduction in variance. At higher values for s, the bias increases and is not offset by further reduction in variance; increaing the test RSS. (This assumes the model is actually suffering from high variance initially..)\n",
    "\n",
    "(c) Repeat (a) for variance.\n",
    "\n",
    "iv. Steadily decrease\n",
    "\n",
    "- We expect the variance to decrease (possibly rapidly initially) and then flatten out.\n",
    "\n",
    "(d) Repeat (a) for (squared) bias.\n",
    "\n",
    "iii. Steadily increase.\n",
    "\n",
    "- We expect the bias to increase steadily (possibly slowly initially) and then possibly increase more rapidly.\n",
    "\n",
    "(e) Repeat (a) for the irreducible error.\n",
    "\n",
    "v. Remain constant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Suppose we estimate the regression coefficients in a linear regression model by minimizing:\n",
    "(Ridge optimisation objective)\n",
    "\n",
    "answers same as for 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give quite dif- ferent coefficient values to correlated variables. We will now explore this property in a very simple setting.\n",
    "\n",
    "Suppose that `n = 2, p = 2`, `x11 = x12`, `x21 = x22`. Furthermore, suppose that `y1+y2 = 0` and `x11 + x21 = 0` and `x12 + x22 =0`,so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: `β_hat_0 = 0.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Write out the ridge regression optimization problem in this set- ting. (b) Argue that in this setting, the ridge coefficient estimates satisfy βˆ 1 = βˆ 2 .\n",
    "\n",
    "[Q5ab.JPG](Q5ab.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Write out the lasso optimization problem in this setting. (d) Argue that in this setting, the lasso coefficients βˆ1 and βˆ2 are not unique—in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
