{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision as tv\n",
    "import torch as torch\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data - CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# From the pytorch docs:\n",
    "# All pre-trained models expect input images normalized in the same way,\n",
    "# i.e. mini-batches of 3-channel RGB images of shape (3 x H x W),\n",
    "# where H and W are expected to be at least 224. The images have to be\n",
    "# loaded in to a range of [0, 1] and then normalized using\n",
    "# mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
    "\n",
    "normalize = tv.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "tfm = tv.transforms.Compose([tv.transforms.RandomResizedCrop(224),\n",
    "                                tv.transforms.RandomHorizontalFlip(),\n",
    "                                tv.transforms.ToTensor(),\n",
    "                                normalize])\n",
    "\n",
    "trainset = tv.datasets.CIFAR10('./data', train=True, download=True, transform=tfm)\n",
    "testset = tv.datasets.CIFAR10('./data', train=False, download=True, transform=tfm)\n",
    "\n",
    "batch_size = 4\n",
    "n_work = 1\n",
    "learn_rate = 1e-3\n",
    "epochs = 1\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=n_work)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=n_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "a, b = (next(iter(trainloader)))\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    ''' reduce the learning rate by a factor of 10 every 30 epochs'''\n",
    "    lr_factor = 0.1 ** (epoch // 30)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = learn_rate * lr_factor\n",
    "        \n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    _, preds = output.topk(maxk, 1, True, True)\n",
    "    topk_targets = target.view(-1, 1).expand_as(preds) \n",
    "    correct = topk_targets == preds\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        num_correct = correct[:,:k].sum().item()\n",
    "        res.append(100 * num_correct / correct.size(0))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tickers_description(tickers):\n",
    "    dt, bt, t1a, t5a, loss = tickers['data_time'], tickers['batch_time'], tickers['top1_accuracy'], tickers['top5_accuracy'], tickers['loss']\n",
    "    avg = lambda t: t[0] / t[1]\n",
    "    return f' data time: {avg(dt)} \\\n",
    "             \\n batch time: {avg(bt)} \\\n",
    "             \\n top1 accuracy: {avg(t1a)} \\\n",
    "             \\n top5 accuracy: {avg(t5a)} \\\n",
    "             \\n loss: {avg(loss)}\\n'\n",
    "\n",
    "def train_epoch(trainloader, model, criterion, optimizer, print_freq=100):\n",
    "    ''' run a single epoch of training'''\n",
    "    model.train()\n",
    "    tickers = {'data_time' : torch.zeros(2),\n",
    "               'batch_time' : torch.zeros(2),\n",
    "               'top1_accuracy' : torch.zeros(2),\n",
    "               'top5_accuracy' : torch.zeros(2),\n",
    "               'loss' : torch.zeros(2)}\n",
    "    end = time.time()\n",
    "    for idx, (input, target) in enumerate(trainloader):\n",
    "        tickers['data_time'] += torch.tensor([time.time() - end, 1])\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        acc1, acc5 = accuracy(output, target,(1,5))\n",
    "        tickers['top1_accuracy'] += torch.tensor([acc1, 1])\n",
    "        tickers['top5_faccuracy'] += torch.tensor([acc5, 1])\n",
    "        tickers['loss'] += torch.tensor([loss.data, 1])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tickers['batch_time'] += torch.tensor([time.time() - end, 1])\n",
    "        end = time.time()\n",
    "        bs = input.size(0)\n",
    "        if idx % print_freq == 0:\n",
    "            print(train_tickers_description(tickers))\n",
    "        #if idx > 500: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tickers_description(tickers):\n",
    "    bt, t1a, t5a = tickers['batch_time'], tickers['top1_accuracy'], tickers['top5_accuracy']\n",
    "    avg = lambda t: t[0] / t[1]\n",
    "    return f' batch time: {avg(bt)} \\\n",
    "             \\n top1 accuracy: {avg(t1a)} \\\n",
    "             \\n top5 accuracy: {avg(t5a)} \\n'\n",
    "\n",
    "def test(testloader, model, criterion, print_freq=100):\n",
    "    ''' evaluate the model'''\n",
    "    model.eval()\n",
    "    tickers = {'data_time' : torch.zeros(2),\n",
    "               'batch_time' : torch.zeros(2),\n",
    "               'top1_accuracy' : torch.zeros(2),\n",
    "               'top5_accuracy' : torch.zeros(2),\n",
    "               'loss' : torch.zeros(2)}\n",
    "    end = time.time()\n",
    "    for idx, (input, target) in enumerate(testloader):\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        acc1, acc5 = accuracy(output, target, (1, 5))\n",
    "        tickers['top1_accuracy'] += torch.tensor([acc1, 1])\n",
    "        tickers['top5_accuracy'] += torch.tensor([acc5, 1])\n",
    "        tickers['batch_time'] += torch.tensor([time.time() - end, 1])\n",
    "        end = time.time()\n",
    "        bs = input.size(0)\n",
    "        if idx % print_freq == 0:\n",
    "            print(test_tickers_description(tickers))\n",
    "    t1a = tickers['top5_accuracy']\n",
    "    return t1a[0] / t1a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, epochs):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), learn_rate, momentum=0.9, weight_decay=1e-4)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f'Epoch: {epoch} \\n')\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        train_epoch(trainloader, model, criterion, optimizer)\n",
    "        top1acc = test(testloader, model, criterion)\n",
    "        print(top1acc.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n",
      "\n",
      " data time: 0.030340909957885742              \n",
      " batch time: 0.0606081485748291              \n",
      " top1 accuracy: 25.0              \n",
      " top5 accuracy: 75.0              \n",
      " loss: 2.1898183822631836\n",
      "\n",
      " data time: 0.0008212788379751146              \n",
      " batch time: 0.010105160996317863              \n",
      " top1 accuracy: 13.613861083984375              \n",
      " top5 accuracy: 63.613861083984375              \n",
      " loss: 213.57818603515625\n",
      "\n",
      " data time: 0.0006711803143844008              \n",
      " batch time: 0.009446665644645691              \n",
      " top1 accuracy: 15.049751281738281              \n",
      " top5 accuracy: 62.68656539916992              \n",
      " loss: 250.29530334472656\n",
      "\n",
      " data time: 0.0006188348052091897              \n",
      " batch time: 0.009142698720097542              \n",
      " top1 accuracy: 15.282392501831055              \n",
      " top5 accuracy: 62.458473205566406              \n",
      " loss: 266.6116943359375\n",
      "\n",
      " data time: 0.0005924731376580894              \n",
      " batch time: 0.008971589617431164              \n",
      " top1 accuracy: 15.835411071777344              \n",
      " top5 accuracy: 61.59600830078125              \n",
      " loss: 277.2897644042969\n",
      "\n",
      " data time: 0.000575828948058188              \n",
      " batch time: 0.008901120163500309              \n",
      " top1 accuracy: 15.918163299560547              \n",
      " top5 accuracy: 61.177642822265625              \n",
      " loss: 283.6082763671875\n",
      "\n",
      " batch time: 0.035479068756103516              \n",
      " top1 accuracy: 0.0              \n",
      " top5 accuracy: 75.0 \n",
      "\n",
      " batch time: 0.007400305010378361              \n",
      " top1 accuracy: 15.346534729003906              \n",
      " top5 accuracy: 65.09900665283203 \n",
      "\n",
      " batch time: 0.007267558015882969              \n",
      " top1 accuracy: 17.786069869995117              \n",
      " top5 accuracy: 64.80099487304688 \n",
      "\n",
      " batch time: 0.007241586688905954              \n",
      " top1 accuracy: 18.521595001220703              \n",
      " top5 accuracy: 63.953487396240234 \n",
      "\n",
      " batch time: 0.007223503664135933              \n",
      " top1 accuracy: 18.26683235168457              \n",
      " top5 accuracy: 64.27680969238281 \n",
      "\n",
      " batch time: 0.007204892113804817              \n",
      " top1 accuracy: 18.612773895263672              \n",
      " top5 accuracy: 64.67066192626953 \n",
      "\n",
      " batch time: 0.007189436350017786              \n",
      " top1 accuracy: 18.344425201416016              \n",
      " top5 accuracy: 64.80865478515625 \n",
      "\n",
      " batch time: 0.007183491718024015              \n",
      " top1 accuracy: 18.188302993774414              \n",
      " top5 accuracy: 64.94293975830078 \n",
      "\n",
      " batch time: 0.007179542910307646              \n",
      " top1 accuracy: 17.66541862487793              \n",
      " top5 accuracy: 64.63795471191406 \n",
      "\n",
      " batch time: 0.007185994181782007              \n",
      " top1 accuracy: 17.980022430419922              \n",
      " top5 accuracy: 64.650390625 \n",
      "\n",
      " batch time: 0.007181117311120033              \n",
      " top1 accuracy: 18.156843185424805              \n",
      " top5 accuracy: 64.53546142578125 \n",
      "\n",
      " batch time: 0.007174400147050619              \n",
      " top1 accuracy: 18.278837203979492              \n",
      " top5 accuracy: 64.55494689941406 \n",
      "\n",
      " batch time: 0.007172238547354937              \n",
      " top1 accuracy: 18.6094913482666              \n",
      " top5 accuracy: 64.5711898803711 \n",
      "\n",
      " batch time: 0.0071701668202877045              \n",
      " top1 accuracy: 18.69715690612793              \n",
      " top5 accuracy: 65.00768280029297 \n",
      "\n",
      " batch time: 0.0071679046377539635              \n",
      " top1 accuracy: 18.986438751220703              \n",
      " top5 accuracy: 65.0428237915039 \n",
      "\n",
      " batch time: 0.007168438285589218              \n",
      " top1 accuracy: 19.070619583129883              \n",
      " top5 accuracy: 65.23983764648438 \n",
      "\n",
      " batch time: 0.007168326061218977              \n",
      " top1 accuracy: 19.066207885742188              \n",
      " top5 accuracy: 65.3966293334961 \n",
      "\n",
      " batch time: 0.00716814212501049              \n",
      " top1 accuracy: 18.959436416625977              \n",
      " top5 accuracy: 65.27043151855469 \n",
      "\n",
      " batch time: 0.0071714180521667              \n",
      " top1 accuracy: 18.975568771362305              \n",
      " top5 accuracy: 65.22765350341797 \n",
      "\n",
      " batch time: 0.007172246463596821              \n",
      " top1 accuracy: 18.871646881103516              \n",
      " top5 accuracy: 65.18937683105469 \n",
      "\n",
      " batch time: 0.007171120028942823              \n",
      " top1 accuracy: 18.95302391052246              \n",
      " top5 accuracy: 65.4797592163086 \n",
      "\n",
      " batch time: 0.007169091608375311              \n",
      " top1 accuracy: 18.907663345336914              \n",
      " top5 accuracy: 65.44502258300781 \n",
      "\n",
      " batch time: 0.007168839685618877              \n",
      " top1 accuracy: 18.923215866088867              \n",
      " top5 accuracy: 65.36801147460938 \n",
      "\n",
      " batch time: 0.007170485332608223              \n",
      " top1 accuracy: 18.883094787597656              \n",
      " top5 accuracy: 65.37374877929688 \n",
      "\n",
      " batch time: 0.007167450617998838              \n",
      " top1 accuracy: 18.887964248657227              \n",
      " top5 accuracy: 65.5247802734375 \n",
      "\n",
      "tensor(65.4400)\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(224 * 224 * 3, 10)\n",
    "            \n",
    "    def forward(self, x):        \n",
    "        return self.fc1(x.view(-1, 3 * 224 * 224))\n",
    "    \n",
    "fit(Net(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - [VGG](https://arxiv.org/pdf/1409.1556.pdf) (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vgg_architectures.png](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/pytorch/vgg_architectures.png?raw=true)\n",
    "\n",
    "- Convolution is performed over a 3 x 3 pixel window, with stride 1 (with exception of 1 x 1 in architecture c). The spatial padding of conv. layer input is such that the spatial resolution is preserved\n",
    "after convolution, i.e. the padding is 1 pixel for 3 × 3 conv. layers\n",
    "- Max-pooling is performed over a 2 × 2 pixel window, with stride 2. Making the side of the spatial input to first fc layer = $\\dfrac{224}{2^ 5} = 7$\n",
    "- All hidden layers are equipped with the rectification non-linearity.\n",
    "- Dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_layers):\n",
    "        super().__init__()\n",
    "        self.conv_layers = conv_layers\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512 * 7 * 7, 4096),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(4096, 4096),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(4096, 1000)\n",
    "            # omitting the final softmax layer\n",
    "            # see discussion here - https://discuss.pytorch.org/t/vgg-output-layer-no-softmax/9273/4\n",
    "            # .. long and short of it is that pytorch's CrossEntropy loss implementation includes\n",
    "            # a log softmax - https://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_layers(x)\n",
    "    \n",
    "archs = {'A' : [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "         'B' : [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "         'D' : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "         'E' : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']}\n",
    "\n",
    "def make_layers(layer_spec):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for l in layer_spec:\n",
    "        if l == 'M':\n",
    "            layers.append(torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        else:\n",
    "            layers.append(torch.nn.Conv2d(in_channels, l, kernel_size=3, padding=1))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            in_channels = l\n",
    "            \n",
    "    return torch.nn.Sequential(*layers)\n",
    "\n",
    "def vgg11():\n",
    "    return VGG(make_layers(archs['A']))\n",
    "\n",
    "def vgg13():\n",
    "    return VGG(make_layers(archs['B']))\n",
    "\n",
    "def vgg16():\n",
    "    return VGG(make_layers(archs['D']))\n",
    "\n",
    "def vgg16():\n",
    "    return VGG(make_layers(archs['E']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit(vgg11(), 1)\n",
    "#fit(tv.models.vgg11(), 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
