{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision as tv\n",
    "import torch as torch\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data - CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# From the pytorch docs:\n",
    "# All pre-trained models expect input images normalized in the same way,\n",
    "# i.e. mini-batches of 3-channel RGB images of shape (3 x H x W),\n",
    "# where H and W are expected to be at least 224. The images have to be\n",
    "# loaded in to a range of [0, 1] and then normalized using\n",
    "# mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
    "\n",
    "normalize = tv.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "tfm = tv.transforms.Compose([tv.transforms.RandomResizedCrop(224),\n",
    "                                tv.transforms.RandomHorizontalFlip(),\n",
    "                                tv.transforms.ToTensor(),\n",
    "                                normalize])\n",
    "\n",
    "trainset = tv.datasets.CIFAR10('./data', train=True, download=True, transform=tfm)\n",
    "testset = tv.datasets.CIFAR10('./data', train=False, download=True, transform=tfm)\n",
    "\n",
    "batch_size = 4\n",
    "n_work = 1\n",
    "learn_rate = 1e-3\n",
    "epochs = 1\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=n_work)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=n_work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    ''' reduce the learning rate by a factor of 10 every 30 epochs'''\n",
    "    lr_factor = 0.1 ** (epoch // 30)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = learn_rate * lr_factor\n",
    "        \n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    _, preds = output.topk(maxk, 1, True, True)\n",
    "    topk_targets = target.view(-1, 1).expand_as(preds) \n",
    "    correct = topk_targets == preds\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        num_correct = correct[:,:k].sum().item()\n",
    "        res.append(100 * num_correct / correct.size(0))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tickers_description(tickers):\n",
    "    dt, bt, t1a, t5a, loss = tickers['data_time'], tickers['batch_time'], tickers['top1_accuracy'], tickers['top5_accuracy'], tickers['loss']\n",
    "    avg = lambda t: t[0] / t[1]\n",
    "    return f' data time: {avg(dt)} \\\n",
    "             \\n batch time: {avg(bt)} \\\n",
    "             \\n top1 accuracy: {avg(t1a)} \\\n",
    "             \\n top5 accuracy: {avg(t5a)} \\\n",
    "             \\n loss: {avg(loss)}\\n'\n",
    "\n",
    "def train_epoch(trainloader, model, criterion, optimizer, print_freq=100):\n",
    "    ''' run a single epoch of training'''\n",
    "    model.train()\n",
    "    tickers = {'data_time' : torch.zeros(2),\n",
    "               'batch_time' : torch.zeros(2),\n",
    "               'top1_accuracy' : torch.zeros(2),\n",
    "               'top5_accuracy' : torch.zeros(2),\n",
    "               'loss' : torch.zeros(2)}\n",
    "    end = time.time()\n",
    "    for idx, (input, target) in enumerate(trainloader):\n",
    "        tickers['data_time'] += torch.tensor([time.time() - end, 1])\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        acc1, acc5 = accuracy(output, target,(1,5))\n",
    "        tickers['top1_accuracy'] += torch.tensor([acc1, 1])\n",
    "        tickers['top5_accuracy'] += torch.tensor([acc5, 1])\n",
    "        tickers['loss'] += torch.tensor([loss.data, 1])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tickers['batch_time'] += torch.tensor([time.time() - end, 1])\n",
    "        end = time.time()\n",
    "        bs = input.size(0)\n",
    "        if idx % print_freq == 0:\n",
    "            print(train_tickers_description(tickers))\n",
    "        #if idx > 500: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tickers_description(tickers):\n",
    "    bt, t1a, t5a = tickers['batch_time'], tickers['top1_accuracy'], tickers['top5_accuracy']\n",
    "    avg = lambda t: t[0] / t[1]\n",
    "    return f' batch time: {avg(bt)} \\\n",
    "             \\n top1 accuracy: {avg(t1a)} \\\n",
    "             \\n top5 accuracy: {avg(t5a)} \\n'\n",
    "\n",
    "def test(testloader, model, criterion, print_freq=100):\n",
    "    ''' evaluate the model'''\n",
    "    model.eval()\n",
    "    tickers = {'data_time' : torch.zeros(2),\n",
    "               'batch_time' : torch.zeros(2),\n",
    "               'top1_accuracy' : torch.zeros(2),\n",
    "               'top5_accuracy' : torch.zeros(2),\n",
    "               'loss' : torch.zeros(2)}\n",
    "    end = time.time()\n",
    "    for idx, (input, target) in enumerate(testloader):\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        acc1, acc5 = accuracy(output, target, (1, 5))\n",
    "        tickers['top1_accuracy'] += torch.tensor([acc1, 1])\n",
    "        tickers['top5_accuracy'] += torch.tensor([acc5, 1])\n",
    "        tickers['batch_time'] += torch.tensor([time.time() - end, 1])\n",
    "        end = time.time()\n",
    "        bs = input.size(0)\n",
    "        if idx % print_freq == 0:\n",
    "            print(test_tickers_description(tickers))\n",
    "    t1a = tickers['top5_accuracy']\n",
    "    return t1a[0] / t1a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, epochs):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), learn_rate, momentum=0.9, weight_decay=1e-4)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f'Epoch: {epoch} \\n')\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        train_epoch(trainloader, model, criterion, optimizer)\n",
    "        top1acc = test(testloader, model, criterion)\n",
    "        print(top1acc.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n",
      "\n",
      " data time: 0.026875019073486328              \n",
      " batch time: 0.054234981536865234              \n",
      " top1 accuracy: 0.0              \n",
      " top5 accuracy: 25.0              \n",
      " loss: 2.8279504776000977\n",
      "\n",
      " data time: 0.0007960395305417478              \n",
      " batch time: 0.009950656443834305              \n",
      " top1 accuracy: 14.10891056060791              \n",
      " top5 accuracy: 60.14851379394531              \n",
      " loss: 224.140869140625\n",
      "\n",
      " data time: 0.000675041286740452              \n",
      " batch time: 0.009447149001061916              \n",
      " top1 accuracy: 15.796019554138184              \n",
      " top5 accuracy: 60.199005126953125              \n",
      " loss: 252.91220092773438\n",
      "\n",
      " data time: 0.0006249228026717901              \n",
      " batch time: 0.009144889190793037              \n",
      " top1 accuracy: 16.02989959716797              \n",
      " top5 accuracy: 61.627906799316406              \n",
      " loss: 270.8465270996094\n",
      "\n",
      " data time: 0.00059844134375453              \n",
      " batch time: 0.009017663076519966              \n",
      " top1 accuracy: 15.586034774780273              \n",
      " top5 accuracy: 62.344139099121094              \n",
      " loss: 283.5514221191406\n",
      "\n",
      " data time: 0.000581828411668539              \n",
      " batch time: 0.008986896835267544              \n",
      " top1 accuracy: 15.818363189697266              \n",
      " top5 accuracy: 61.72654724121094              \n",
      " loss: 290.1764831542969\n",
      "\n",
      " batch time: 0.036959171295166016              \n",
      " top1 accuracy: 50.0              \n",
      " top5 accuracy: 75.0 \n",
      "\n",
      " batch time: 0.007442358881235123              \n",
      " top1 accuracy: 16.336633682250977              \n",
      " top5 accuracy: 62.87128829956055 \n",
      "\n",
      " batch time: 0.007341488264501095              \n",
      " top1 accuracy: 17.53731346130371              \n",
      " top5 accuracy: 62.31343460083008 \n",
      "\n",
      " batch time: 0.007324082311242819              \n",
      " top1 accuracy: 17.691030502319336              \n",
      " top5 accuracy: 62.20930099487305 \n",
      "\n",
      " batch time: 0.007302079349756241              \n",
      " top1 accuracy: 17.89276885986328              \n",
      " top5 accuracy: 63.216957092285156 \n",
      "\n",
      " batch time: 0.007285723928362131              \n",
      " top1 accuracy: 17.514970779418945              \n",
      " top5 accuracy: 63.27345275878906 \n",
      "\n",
      " batch time: 0.007264833897352219              \n",
      " top1 accuracy: 17.304492950439453              \n",
      " top5 accuracy: 64.01830291748047 \n",
      "\n",
      " batch time: 0.007256426382809877              \n",
      " top1 accuracy: 17.475034713745117              \n",
      " top5 accuracy: 64.01569366455078 \n",
      "\n",
      " batch time: 0.007246100809425116              \n",
      " top1 accuracy: 17.509363174438477              \n",
      " top5 accuracy: 63.63920211791992 \n",
      "\n",
      " batch time: 0.007241628598421812              \n",
      " top1 accuracy: 17.397336959838867              \n",
      " top5 accuracy: 64.01220703125 \n",
      "\n",
      " batch time: 0.00722624221816659              \n",
      " top1 accuracy: 17.532466888427734              \n",
      " top5 accuracy: 64.23576354980469 \n",
      "\n",
      " batch time: 0.00722385011613369              \n",
      " top1 accuracy: 17.847412109375              \n",
      " top5 accuracy: 64.30517578125 \n",
      "\n",
      " batch time: 0.007228343281894922              \n",
      " top1 accuracy: 17.672773361206055              \n",
      " top5 accuracy: 64.34221649169922 \n",
      "\n",
      " batch time: 0.007229112088680267              \n",
      " top1 accuracy: 17.755573272705078              \n",
      " top5 accuracy: 64.52729034423828 \n",
      "\n",
      " batch time: 0.00722777983173728              \n",
      " top1 accuracy: 17.73733139038086              \n",
      " top5 accuracy: 64.52533721923828 \n",
      "\n",
      " batch time: 0.007220317609608173              \n",
      " top1 accuracy: 17.604930877685547              \n",
      " top5 accuracy: 64.52365112304688 \n",
      "\n",
      " batch time: 0.007221047766506672              \n",
      " top1 accuracy: 17.56714630126953              \n",
      " top5 accuracy: 64.39725494384766 \n",
      "\n",
      " batch time: 0.007220256142318249              \n",
      " top1 accuracy: 17.47501564025879              \n",
      " top5 accuracy: 64.32980346679688 \n",
      "\n",
      " batch time: 0.007213760167360306              \n",
      " top1 accuracy: 17.448638916015625              \n",
      " top5 accuracy: 64.3392562866211 \n",
      "\n",
      " batch time: 0.0072168875485658646              \n",
      " top1 accuracy: 17.5170955657959              \n",
      " top5 accuracy: 64.46607208251953 \n",
      "\n",
      " batch time: 0.007215320598334074              \n",
      " top1 accuracy: 17.616191864013672              \n",
      " top5 accuracy: 64.39280700683594 \n",
      "\n",
      " batch time: 0.007224861532449722              \n",
      " top1 accuracy: 17.491670608520508              \n",
      " top5 accuracy: 64.32650756835938 \n",
      "\n",
      " batch time: 0.0072218202985823154              \n",
      " top1 accuracy: 17.44661521911621              \n",
      " top5 accuracy: 64.39118957519531 \n",
      "\n",
      " batch time: 0.0072194174863398075              \n",
      " top1 accuracy: 17.590177536010742              \n",
      " top5 accuracy: 64.31986236572266 \n",
      "\n",
      " batch time: 0.007218943443149328              \n",
      " top1 accuracy: 17.51353645324707              \n",
      " top5 accuracy: 64.29612731933594 \n",
      "\n",
      "tensor(64.3400)\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(224 * 224 * 3, 10)\n",
    "            \n",
    "    def forward(self, x):        \n",
    "        return self.fc1(x.view(-1, 3 * 224 * 224))\n",
    "    \n",
    "fit(Net(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - [VGG](https://arxiv.org/pdf/1409.1556.pdf) (2015)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vgg_architectures.png](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/pytorch/vgg_architectures.png?raw=true)\n",
    "\n",
    "- Convolution is performed over a 3 x 3 pixel window, with stride 1 (with exception of 1 x 1 in architecture c). The spatial padding of conv. layer input is such that the spatial resolution is preserved\n",
    "after convolution, i.e. the padding is 1 pixel for 3 × 3 conv. layers\n",
    "- Max-pooling is performed over a 2 × 2 pixel window, with stride 2. Making the side of the spatial input to first fc layer = $\\dfrac{224}{2^ 5} = 7$\n",
    "- All hidden layers are equipped with the rectification non-linearity.\n",
    "- Dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_layers):\n",
    "        super().__init__()\n",
    "        self.conv_layers = conv_layers\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512 * 7 * 7, 4096),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(4096, 4096),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(4096, 1000)\n",
    "            # omitting the final softmax layer\n",
    "            # see discussion here - https://discuss.pytorch.org/t/vgg-output-layer-no-softmax/9273/4\n",
    "            # .. long and short of it is that pytorch's CrossEntropy loss implementation includes\n",
    "            # a log softmax - https://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_layers(x)\n",
    "    \n",
    "archs = {'A' : [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "         'B' : [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "         'D' : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "         'E' : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']}\n",
    "\n",
    "def make_layers(layer_spec):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for l in layer_spec:\n",
    "        if l == 'M':\n",
    "            layers.append(torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        else:\n",
    "            layers.append(torch.nn.Conv2d(in_channels, l, kernel_size=3, padding=1))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            in_channels = l\n",
    "            \n",
    "    return torch.nn.Sequential(*layers)\n",
    "\n",
    "def vgg11():\n",
    "    return VGG(make_layers(archs['A']))\n",
    "\n",
    "def vgg13():\n",
    "    return VGG(make_layers(archs['B']))\n",
    "\n",
    "def vgg16():\n",
    "    return VGG(make_layers(archs['D']))\n",
    "\n",
    "def vgg16():\n",
    "    return VGG(make_layers(archs['E']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit(vgg11(), 1)\n",
    "#fit(tv.models.vgg11(), 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
