<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>curriculum</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="../css/markdown.css">
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>
<a style="padding-right:20px;" href="./index.html">Home</a> <a style="padding-right:20px;" href="./curriculum.html">Curriculum</a> <a style="padding-right:20px;" href="./learning_log.html">Learning Log</a> <a style="padding-right:20px;" href="./bookshelf.html">Bookshelf</a>
</p>
<h2 id="curriculum">Curriculum</h2>
<p>This curriculum is inspired by phase one from <a href="http://karlrosaen.com/ml/">Karl Rosen’s curriculum</a></p>
<p>I like the idea of having two tracks; one applied, one foundational, with a primary linear resource in each.</p>
<h3 id="track-1-applied-ml">Track 1: Applied ML</h3>
<p>PML: <a href="https://www.amazon.co.uk/Python-Machine-Learning-Sebastian-Raschka/dp/1783555130">Python Machine Learning</a></p>
<p>For each chapter I intend to publish my notes and working Jupyter Notebooks / python code that exercise the key ideas. I intend to cross reference the material as I go with the Andrew Ng courses I’ve already completed and with other resources that I find along the way. I’ll also be looking for overlap with the foundational track where possible, in addition to diving deeper in patches where the content errs on “plug and chug”.</p>
<ul>
<li>PML: 1. Giving Computers the Ability to Learn From Data</li>
<li>PML: 2. Training Machine Learning Algorithms For Classification</li>
<li>PML: 3. A Tour of Machine Learning Classifiers Using Scikit-learn</li>
<li>PML: 4. Building Good Training Sets- Data Preprocessing</li>
<li>PML: 5. Compressing Data via Dimensionality Reduction</li>
<li>PML: 6. Learning Best Practices for Model Evaluation and Hyperparameter Tuning</li>
<li>PML: 7. Combining Different Models for Ensemble Learning</li>
<li>PML: 10. Predicting Continuous Target Variables with Regression Analysis</li>
<li>PML: 11. Working With Unlabeled Data - Clustering Analysis</li>
<li><p>PML: 12. Training Artificial Neural Networks for Image Recognition.</p></li>
<li>Kaggle: <a href="https://www.kaggle.com/c/titanic/overview">1.Titanic: Machine Learning from Disaster</a></li>
<li>Kaggle: <a href="https://www.kaggle.com/c/forest-cover-type-prediction">2.Forest Cover Type Prediction</a></li>
<li><p>Kaggle: <a href="https://www.kaggle.com/c/predicting-red-hat-business-value">3.Predicting Red Hat Business Value</a></p></li>
<li>OSS contributions: TBD</li>
<li><p>Side project: TBD</p></li>
</ul>
<h4 id="artefacts-of-achievement">Artefacts of achievement:</h4>
<ul>
<li>Well documented code that excercises the key ideas.</li>
<li>Daily learning log entries.</li>
<li>Jupyter notebooks / well documented code for Kaggle contest entries.</li>
<li>? OSS contributions</li>
<li>? Side project</li>
</ul>
<h3 id="track-2-probability-and-statistics">Track 2: Probability and statistics</h3>
<p>SPS: <a href="https://lagunita.stanford.edu/courses/course-v1:OLI+ProbStat+Open_Jan2017/about">Stanford’s Free Probability and Statistics Course</a> I intend to cross reference the material as I go with other resources including: Kahn Academy, MathMonk (YouTube) and other resources I find along the way.</p>
<ul>
<li>SPS: 1.Introduction</li>
<li>SPS: 2.Exploratory Data Analysis: Examining Distributions</li>
<li>SPS: 3.Exploratory Data Analysis: Examining Relationships</li>
<li>SPS: 4.Producing Data: Sampling</li>
<li>SPS: 5.Producing Data: Designing Studies</li>
<li>SPS: 6.Probability: Introduction</li>
<li>SPS: 7.Probability: Finding Probability of Events</li>
<li>SPS: 8.Probability: Conditional Probability and Independence</li>
<li>SPS: 9.Probability: Discrete Random Variables</li>
<li>SPS: 10.Probability: Continuous Random Variables</li>
<li>SPS: 11.Probability: Sampling Distributions</li>
<li>SPS: 12.Inference: Estimation</li>
<li>SPS: 13.Inference: Hypothesis Testing Overview</li>
<li>SPS: 14.Inference: Hypothesis Testing for the Population Proportion</li>
<li>SPS: 15.Inference: Hypothesis Testing for the Population Mean</li>
<li>SPS: 16.Inference: Relationships C -&gt; Q</li>
<li>SPS: 17.Inference: Relationships C -&gt; C</li>
<li>SPS: 18.Inference: Relationships Q -&gt; Q</li>
</ul>
<p>AOS: <a href="https://www.amazon.co.uk/All-Statistics-Statistical-Inference-Springer/dp/0387402721">All of statistics</a></p>
<p>I gather that this text is ruthlessly dry, providing consise treatment of the key ideas in a sensible order, with plenty of examples.</p>
<p>I intend to use it as a roadmap and expect to jump out to other resources and do cross referencing for 2nd explanations. I expect to make use of MathMonk’s <a href="https://www.youtube.com/playlist?list=PL17567A1A3F5DB5E4">Probability Primer</a>. I’ve ordered <a href="https://www.amazon.co.uk/gp/product/9812703713/ref=oh_aui_detailpage_o01_s00?ie=UTF8&amp;psc=1">A first look at rigorous probability theory</a> for bedside reading. I’m eying up <a href="https://www.amazon.co.uk/Theory-Probability-Explorations-Applications/dp/1107024471">The Theory of Probability: explorations and Applications</a> as a second reference text to accompany Wasserman.</p>
<ul>
<li>AOS: 1. Probability</li>
<li>AOS: 2. Random Variables</li>
<li>AOS: 3. Expectation</li>
</ul>
<p>Looking through <a href="http://karlrosaen.com/ml/learning-log/">karl’s learning log</a> (see my notes <a href="https://github.com/coxy1989/mlsabbatical/blob/master/notes/01-karl-rosaen-learning-log-notes.md">here</a>) it looks like this is the most lonely part; an area where more hand holding may have sped things up. When I’m done with the Stanford Stats course and begin approaching Wasserman I’ll have a low threshold for seeking another MOOC type experience to lubricate this track if I percieve that I’m not moving as fast as I should be.</p>
<h4 id="artefacts-of-achievement-1">Artefacts of achievement:</h4>
<ul>
<li>Well documented code/notes/proofs that excercise the key ideas.</li>
<li>HN publication of a notebook on a key idea - <a href="http://karlrosaen.com/ml/learning-log/2016-05-17/">see karls</a></li>
<li>Homeworks: TBD - <a href="http://karlrosaen.com/ml/hw/">see karls</a></li>
</ul>
</body>
</html>
