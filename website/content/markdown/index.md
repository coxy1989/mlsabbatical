Check out the [Learning Log](https://www.coxy1989.com/learning_log.html)

## Welcome

Hello, I'm [Jamie](https://github.com/coxy1989). I've created this site to track my machine learning studies, the site is statically hosted on S3 - you can find the source code [here](https://github.com/coxy1989/mlsabbatical).

### Starting point

- 3 years professional software engineering: variety of domains/platforms/languages.
- Working maths knowledge.
- ML feet are wet: I've gone through various books, blogs and tutorials and [messed around](https://github.com/coxy1989/clj_mnist).
- Domain knowledge of medicine.


## Core Modules

- Introduction to Machine Learning
	- Course: [Machine Learning](https://www.coursera.org/learn/machine-learning) from Stanford by Andrew Ng
		
- Supervised Learning
	- Course: [Statistical Learning]() from Stanford by Trevor Hastie and Rob Tibshirani
	- Textbooks:
		- [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)
	   	- [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)

- Mathematics for Machine Learning
	- Course: [Mathematics for Machine Learning](https://www.coursera.org/specializations/mathematics-machine-learning) from Imperial College London
	- Textbook: [Mathematical Methods in the Physical Sciences](https://www.amazon.co.uk/Mathematical-Methods-Physical-Sciences-Mary/dp/0471365807) 
	- Other Resources:
		-  [Pavel Grinfeld's Linear Algebra via lem.ma](https://www.lem.ma/home).
		-  [Gilbert Strang's Linear Algebra via opencourseware](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/)
		- [Symbolab](https://www.symbolab.com/)
		- [Khan Academy](https://www.khanacademy.org/math/)
		- [Murphy's MLPP](https://www.amazon.co.uk/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020) 

### Optional Modules

- *Bayesian Inference / Graphical Models / Probabalistic ML*
	- TBC

- *Convex Optimization*
	- Course: 
		- [Convex Optimization](http://www.stat.cmu.edu/~ryantibs/convexopt/) from Carnigie Mellon by Ryan Tibsharani 
		- [Convex Optimization](https://lagunita.stanford.edu/courses/Engineering/CVX101/Winter2014/info) from Stanford by Stephen Boyd
	- Textbooks:
		- 	[Boyd's Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- *Reinforcement Learning*
	- Course: [Introduction to Reinforcement Learning](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ) from DeepMind by David Silver
	- Textbooks: 
		- [Reinforcment Learning (Sutton)](https://www.amazon.co.uk/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262193981)
		- [Algorithms for Reinforcement Learning (Szepesvari)](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf)
- *Deep Learning*
	- TBC
- *Kaggling*
	- TBC


## Supervised Learning

*Assignments and other notebooks I've produced along the way*

### Notebooks

- Statistical Learning Introduction
	- ðŸ“–[The Curse of Dimensionality](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch2_statistical_learning/curse_of_dimensionality.ipynb)
	- ðŸ“–[Exercises - Conceptual](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch2_statistical_learning/exercises.ipynb)

- Linear Regression
	- ðŸ“–[Algorithm: Linear Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/linear_regression.ipynb)
	- ðŸ“– [Exercises - Conceptual](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch3_statistical_learning/exercises_conceptual.ipynb)
	- ðŸ“– [Exercises - Applied](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch3_statistical_learning/exercises_applied.ipynb)
	
- Classification
	- ðŸ“–[Algorithm: Logistic Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/logistic_regression.ipynb)
	- ðŸ“–[Algorithm: Linear Discriminant Analysis](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/linear_discriminant_analysis.ipynb)
	- ðŸ“–[Algorithm: Quadratic Discriminant Analysis](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/quadratic_discriminant_analysis.ipynb)
	- ðŸ“–[Lab](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch4_statistical_learning/lab.ipynb)
	- ðŸ“–[Exercises - Conceptual](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch4_statistical_learning/exercises_conceptual.ipynb)
	- ðŸ“–[Exercises - Applied](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch4_statistical_learning/exercises_applied.ipynb)

- Resampling Methods
	- ðŸ“–[Algorithm: K-Fold Cross Validation](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/cross_validation.ipynb)
	- ðŸ“–[Algorithm: The Bootstrap](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/bootstrap.ipynb)
	- ðŸ“–[Lab](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch5_statistical_learning/lab.ipynb)
	- ðŸ“–[Exercises - Conceptual](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch4_statistical_learning/exercises_conceptual.ipynb)
	- ðŸ“–[Exercises - Applied](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch4_statistical_learning/exercises_applied.ipynb)

- Linear Model Selection and Regularisation
	- ðŸ“–[Algorithm: Best Subset Selection](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/best_subset_selection.ipynb)
	- ðŸ“–[Algorithm: Forward and Backward Stepwise Selection](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/stepwise_selection.ipynb)
	- ðŸ“–[Algorithm: Ridge Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/ridge_regression.ipynb)
	- ðŸ“–*Algorithm: The Lasso*
	- ðŸ“–*Algorithm: Principal Components Regression*
	- ðŸ“–*Algorithm: Partial Least Squares*
	- ðŸ“–[Lab](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch6_statistical_learning/lab.ipynb)
	- ðŸ“–[Exercises - Conceptual](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch5_statistical_learning/exercises_conceptual.ipynb)
	- ðŸ“–[Exercises - Applied](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch5_statistical_learning/exercises_applied.ipynb)

- Moving Beyond Linearity
	- ðŸ“–*Algorithm: Regression Spline* 
	- ðŸ“–*Algorithm: Smoothing Spline* 	
	- ðŸ“–*Algorithm: Local Regression* 
	- ðŸ“–*Algorithm: Generalized Additive Model (GAM)* 
	- ðŸ“–[Lab](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch7_statistical_learning/lab.ipynb)
	- ðŸ“–[Exercises - Conceptual](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch7_statistical_learning/conceptual.ipynb)
	- ðŸ“–[Exercises - Applied](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch7_statistical_learning/applied.ipynb)

- Tree Based Methods
	- ðŸ“–[Algorithms: Regression Trees, Bagging and Random Forests](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/regression_trees_ensemble.ipynb)
	- ðŸ“–*Algorithms: Classification Trees, Bagging and Random Forests* 
	- ðŸ“–*Algorithm: Gradient Boosting* 
	- ðŸ“–*Algorithm: AdaBoost* 
	- ðŸ“–[Lab](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch8_statistical_learning/lab.ipynb)
	- ðŸ“–[Exercises - Conceptual](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch8_statistical_learning/conceptual.ipynb)
	- ðŸ“–[Exercises - Applied](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch8_statistical_learning/applied.ipynb)

	
- Support Vector Machines
 	- ðŸ“–[Algorithm: Perceptron](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/rosenblatt_perceptron.ipynb)
	- ðŸ“–*Algorithm: Support Vector Classifier* 
	- ðŸ“–*Algorithm: Support Vector Machine* 
	- ðŸ“–[Lab](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch9_statistical_learning/lab.ipynb)
	- ðŸ“–[Exercises - Conceptual](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch9_statistical_learning/conceptual.ipynb)
	- ðŸ“–[Exercises - Applied](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch9_statistical_learning/applied.ipynb)

- Unsupervised Learning
	- ðŸ“–[Algorithm: PCA](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/pca.ipynb)
	- ðŸ“–[Algorithm: K-Means](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/k_means.ipynb)
	- ðŸ“–[Lab](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch10_statistical_learning/lab.ipynb)
	- ðŸ“–[Exercises - Conceptual](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch10_statistical_learning/conceptual.ipynb)
	- ðŸ“–[Exercises - Applied](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch10_statistical_learning/applied.ipynb)


## Mathematics for Machine Learning

*FWIW - you get a certificate of completion, mine is [here](https://www.coursera.org/account/accomplishments/specialization/ARMLMTNPZJTD)*

Disclaimer - I did not *create* these notebooks in this section, though I did *complete* them - they are provided by Coursera for completion as part of the assessement material for the course.

- ðŸ“–[Transformation and change of basis](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/la_ReflectingBear.ipynb)
- ðŸ“–[Singular matrices](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/la_IdentifyingSpecialMatrices.ipynb)
- ðŸ“–[The Gram-Schmidt process](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/la_GramSchmidtProcess.ipynb)
- ðŸ“–[Power iteration and PageRank](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/la_PageRank.ipynb)
- ðŸ“–[Backpropagation](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/calc_Backpropagation.ipynb)
- ðŸ“–[Gradient Descent](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/calc_Fitting%2Bthe%2Bdistribution%2Bof%2Bheights%2Bdata.ipynb)
- ðŸ“–[Variance, covariance and affine transformation of a dataset](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/pca_week1.ipynb)
- ðŸ“–[Distance and angles in high dimensions](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/pca_week2.ipynb)
- ðŸ“–[Orthogonal Projections](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/pca_week3.ipynb)
- ðŸ“–[Implement Principal Component Analysis](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/pca_week4.ipynb)


## An Introduction to Machine Learning

*All the material is available for free - you have to pay $70 If you want a certificate, so I gave that a miss. I obtained full credit in all the assignments, I've linked to my solutions below.*

- [Octave Lab sheet: Implement Linear Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex1/ex1.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex1/ex1/ex1.m)
- [Octave  Lab sheet: Implement Regularised Linear Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex5/ex5.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex5/ex5)
- [Octave Lab sheet: Implement Logistic Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex2/ex2.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex2/ex2)

- [Octave Lab sheet: Implement One vs. All Logistic Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex3/ex3.pdf)
	- [My Solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex3/ex3)

- [Octave Lab sheet: Implement Backpropagation](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex4/ex4.pdf)
	- [My Solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex4/ex4)

- [Octave Lab sheet: Implement an SVM](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex6/ex6.pdf)
	- [My Solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex6/ex6)

- [Octave Lab sheet: Implement K-means clustering and PCA](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex7/ex7.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex7/ex7)

- [Octave Lab sheet: Implement an anomaly detection algorithm and a recommendation algorithm](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex8/ex8.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex8/ex8)

## Dead Ends

#### Candidate for Unit 3: Statistical Learning

[Foundations of Machine Learning](https://bloomberg.github.io/foml/?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=email&utm_source=Artificial_Intelligence_Weekly_81#home) by Bloomberg ML EDU

- Ridge Regression, Gradient Descent and SGD
	- [Concept Check 1: Statistical Learning Theory](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/bloomberg/downloads/bloomberg_lec_check_1.pdf)
	- [My solutions: written](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/bloomberg/bloomberg_lec_check_1.md)
	- [My solutions: code](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/bloomberg/bloomberg_lec_check_1.py)

Discontinued in favour of [Statistical Learning](https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about) by Trevor Hastie and Rob Tibshirani.