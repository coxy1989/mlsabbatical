Check out the [Learning Log](https://www.coxy1989.com/learning_log.html)

## Welcome

Hello, I'm [Jamie](https://github.com/coxy1989). I've created this site to track my machine learning studies, the site is statically hosted on S3 - you can find the source code [here](https://github.com/coxy1989/mlsabbatical).

### Starting point

- 3 years professional software engineering: variety of domains/platforms/languages.
- Working maths knowledge.
- ML feet are wet: I've gone through various books, blogs and tutorials and [messed around](https://github.com/coxy1989/clj_mnist).
- Domain knowledge of medicine.


## Modules

- Introduction to Machine Learning
	- Course: [Machine Learning](https://www.coursera.org/learn/machine-learning) from Stanford by Andrew Ng
		
- Maths Refresher
	- Courses: 
		- [Mathematics for Machine Learning](https://www.coursera.org/specializations/mathematics-machine-learning) from Imperial College London
		-  [Linear Algebra via lem.ma](https://www.lem.ma/home) by Pavel Grinfeld 
	- Textbooks: 
		- [Mathematical Methods in the Physical Sciences](https://www.amazon.co.uk/Mathematical-Methods-Physical-Sciences-Mary/dp/0471365807) 
		- [Hello Again, Linear Algebra: A Second Look at the Subject through a Collection of Exercises and Solutions](https://www.amazon.co.uk/Hello-Again-Linear-Algebra-Collection-ebook/dp/B00HLW5V9U)
	- Other Resources:		
		-  [Gilbert Strang's Linear Algebra via opencourseware](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/)
		- [Symbolab](https://www.symbolab.com/)
		- [Khan Academy](https://www.khanacademy.org/math/)
		
- Supervised Learning
	- Course: [Statistical Learning](https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about) from Stanford by Trevor Hastie and Rob Tibshirani
	- Textbooks:
		- [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)
	   	- [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)
	   	- [Machine Learning: A Probabalistic Perspective](https://www.amazon.co.uk/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020) 
	- Projects: 
		- [Predicting house prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) in Iowa
		- [Predicting bursts](https://solveitsew.devpost.com/) in South East Water's distribution network
		- [Predicting West Nile virus](https://www.kaggle.com/c/predict-west-nile-virus) in Chicago

- Deep Learning
	- Courses: 
		- [Jeremy Howard's deep learning I](https://course.fast.ai/index.html)
		- [Jeremy Howard's deep learning II](https://course.fast.ai/part2.html)
	- Textbooks:
		- [Deep Learning - Goodfellow, Bengio, Courville](https://www.deeplearningbook.org/)

## Future Study

- *Bayesian Inference & Graphical Models*
	- Courses:
		- [Aubrey Clayton's Logic of Science](https://www.youtube.com/playlist?list=PL9v9IXDsJkktefQzX39wC2YG07vw7DsQ_)
		- [Richard McElrath's Statisitcal Rethinking](https://www.youtube.com/playlist?list=PLDcUM9US4XdM9_N6XUUFrhghGJ4K25bFc)
		- [Daphne Coller's Probabilistic Graphical Models](https://www.coursera.org/specializations/probabilistic-graphical-models)
	- Textbooks:
		- [E.T Jaynes's The Logic of Science](https://bayes.wustl.edu/etj/prob/book.pdf)
		- [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/)

- *Convex Optimization*
	- Courses: 
		- [Convex Optimization](http://www.stat.cmu.edu/~ryantibs/convexopt/) from Carnigie Mellon by Ryan Tibsharani 
		- [Convex Optimization](https://lagunita.stanford.edu/courses/Engineering/CVX101/Winter2014/info) from Stanford by Stephen Boyd
	- Textbooks:
		- 	[Boyd's Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- *Reinforcement Learning*
	- Course: [Introduction to Reinforcement Learning](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ) from DeepMind by David Silver
	- Textbooks: 
		- [Reinforcement Learning (Sutton)](https://www.amazon.co.uk/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262193981)
		- [Algorithms for Reinforcement Learning (Szepesvari)](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf)

## Artefacts

*Assignments and other notebooks I've produced along the way*

### *Supervised Learning*

#### Labs, Conceptual and Applied Excercises

- Statistical Learning Introduction
	- ðŸ“–[Exercises - Conceptual](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch2_statistical_learning/exercises.ipynb)
	- ðŸ“–[The Curse of Dimensionality](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch2_statistical_learning/curse_of_dimensionality.ipynb)

- Linear Regression
	- ðŸ“– [Exercises - Conceptual](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch3_statistical_learning/exercises_conceptual.ipynb)
	- ðŸ“– [Exercises - Applied](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch3_statistical_learning/exercises_applied.ipynb)
	
- Classification
	- ðŸ“–[Exercises - Conceptual](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch4_statistical_learning/exercises_conceptual.ipynb)
	- ðŸ“–[Exercises - Applied](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch4_statistical_learning/exercises_applied.ipynb)
	- ðŸ“–[Lab](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch4_statistical_learning/lab.ipynb)

- Resampling Methods
	- ðŸ“–[Exercises - Conceptual](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch4_statistical_learning/exercises_conceptual.ipynb)
	- ðŸ“–[Exercises - Applied](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch4_statistical_learning/exercises_applied.ipynb)
	- ðŸ“–[Lab](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch5_statistical_learning/lab.ipynb)

- Linear Model Selection and Regularisation
	- ðŸ“–[Exercises - Conceptual](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch5_statistical_learning/exercises_conceptual.ipynb)
	- ðŸ“–[Exercises - Applied](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch5_statistical_learning/exercises_applied.ipynb)
	- ðŸ“–[Lab](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch6_statistical_learning/lab.ipynb)

- Moving Beyond Linearity
	- ðŸ“–[Exercises - Conceptual](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch7_statistical_learning/conceptual.ipynb)
	- ðŸ“–[Exercises - Applied](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch7_statistical_learning/applied.ipynb)
	- ðŸ“–[Lab](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch7_statistical_learning/lab.ipynb)

- Tree Based Methods
	- ðŸ“–[Exercises - Conceptual](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch8_statistical_learning/conceptual.ipynb)
	- ðŸ“–[Exercises - Applied](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch8_statistical_learning/applied.ipynb)
	- ðŸ“–[Lab](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch8_statistical_learning/lab.ipynb)

- Support Vector Machines
	- ðŸ“–[Exercises - Conceptual](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch9_statistical_learning/conceptual.ipynb)
	- ðŸ“–[Exercises - Applied](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch9_statistical_learning/applied.ipynb)
	- ðŸ“–[Lab](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch9_statistical_learning/lab.ipynb)

- Unsupervised Learning
	- ðŸ“–[Exercises - Conceptual](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch10_statistical_learning/conceptual.ipynb)
	- ðŸ“–[Exercises - Applied](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch10_statistical_learning/applied.ipynb)
	- ðŸ“–[Lab](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch10_statistical_learning/lab.ipynb)

#### Algorithms

- ðŸ“–[Linear Regression](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/linear_regression.ipynb)
- ðŸ“–[Logistic Regression](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/logistic_regression.ipynb)
- ðŸ“–[Linear Discriminant Analysis](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/linear_discriminant_analysis.ipynb)
- ðŸ“–[Quadratic Discriminant Analysis](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/quadratic_discriminant_analysis.ipynb)
- ðŸ“–[K-Fold Cross Validation](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/cross_validation.ipynb)
- ðŸ“–[The Bootstrap](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/bootstrap.ipynb)
- ðŸ“–[Best Subset Selection](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/best_subset_selection.ipynb)
- ðŸ“–[Forward and Backward Stepwise Selection](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/stepwise_selection.ipynb)
- ðŸ“–[Ridge Regression](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/ridge_regression.ipynb)
- ðŸ“–[Trees, Bagging, Random Forests and Boosting](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/trees.ipynb)
- ðŸ“–[Perceptron](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/rosenblatt_perceptron.ipynb)
- ðŸ“–[PCA](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/pca.ipynb)
- ðŸ“–[K-Means](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/algorithms/k_means.ipynb)

#### Projects

- [Predicting house prices](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/house_prices/house_prices.ipynb) in Iowa
- [Predicting West Nile virus](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/west_nile_virus/west_nile_virus.ipynb)  in Chicago
- [Predicting bursts](https://docs.google.com/presentation/d/1kaTD7jr0DhJR7IDXVY0lVQ7-jD_r_qhhgqzK9_5nKRc/edit#slide=id.p) in South East Water's distribution network


### *Mathematics for Machine Learning*

*FWIW - you get a certificate of completion from the Imperial course, mine is [here](https://www.coursera.org/account/accomplishments/specialization/ARMLMTNPZJTD)*

Disclaimer - I did not *create* these notebooks in this section, though I did *complete* them - they are provided by Coursera for completion as part of the assessement material for the Imperial course.

- ðŸ“–[Transformation and change of basis](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/imperial/la_ReflectingBear.ipynb)
- ðŸ“–[Singular matrices](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/imperial/la_IdentifyingSpecialMatrices.ipynb)
- ðŸ“–[The Gram-Schmidt process](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/imperial/la_GramSchmidtProcess.ipynb)
- ðŸ“–[Power iteration and PageRank](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/imperial/la_PageRank.ipynb)
- ðŸ“–[Backpropagation](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/imperial/calc_Backpropagation.ipynb)
- ðŸ“–[Gradient Descent](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/imperial/calc_Fitting%2Bthe%2Bdistribution%2Bof%2Bheights%2Bdata.ipynb)
- ðŸ“–[Variance, covariance and affine transformation of a dataset](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/imperial/pca_week1.ipynb)
- ðŸ“–[Distance and angles in high dimensions](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/imperial/pca_week2.ipynb)
- ðŸ“–[Orthogonal Projections](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/imperial/pca_week3.ipynb)
- ðŸ“–[Implement Principal Component Analysis](https://nbviewer.jupyter.org/github/coxy1989/mlsabbatical/blob/master/notebooks/imperial/pca_week4.ipynb)


### *Introduction to Machine Learning*

*All the material for Andrew Ng's course is available for free - you have to pay $70 If you want a certificate, so I gave that a miss. I obtained full credit in all the assignments, I've linked to my solutions below.*

- [Octave Lab sheet: Implement Linear Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex1/ex1.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex1/ex1/ex1.m)
- [Octave  Lab sheet: Implement Regularised Linear Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex5/ex5.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex5/ex5)
- [Octave Lab sheet: Implement Logistic Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex2/ex2.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex2/ex2)

- [Octave Lab sheet: Implement One vs. All Logistic Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex3/ex3.pdf)
	- [My Solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex3/ex3)

- [Octave Lab sheet: Implement Backpropagation](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex4/ex4.pdf)
	- [My Solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex4/ex4)

- [Octave Lab sheet: Implement an SVM](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex6/ex6.pdf)
	- [My Solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex6/ex6)

- [Octave Lab sheet: Implement K-means clustering and PCA](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex7/ex7.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex7/ex7)

- [Octave Lab sheet: Implement an anomaly detection algorithm and a recommendation algorithm](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex8/ex8.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex8/ex8)

## Dead Ends

#### Candidate for Unit 3: Statistical Learning

[Foundations of Machine Learning](https://bloomberg.github.io/foml/?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=email&utm_source=Artificial_Intelligence_Weekly_81#home) by Bloomberg ML EDU

- Ridge Regression, Gradient Descent and SGD
	- [Concept Check 1: Statistical Learning Theory](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/bloomberg/downloads/bloomberg_lec_check_1.pdf)
	- [My solutions: written](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/bloomberg/bloomberg_lec_check_1.md)
	- [My solutions: code](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/bloomberg/bloomberg_lec_check_1.py)

Discontinued in favour of [Statistical Learning](https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about) by Trevor Hastie and Rob Tibshirani.