<p>
  <a style="padding-right:20px;" href="./index.html">Home</a>
  <a style="padding-right:20px;" href="./curriculum.html">Curriculum</a>
  <a href="./learning_log.html">Learning Log</a>
</p>

## Learning Log

Check out the Kanban [here](https://github.com/coxy1989/mlsabbatical/projects/1)


### Monday, 16/07/2018

Morning working on my [curriculum](https://www.coxy1989.com/curriculum.html). Activities have included:

- Reviewing Karl Rosean's [learning log](http://karlrosaen.com/ml/learning-log/) (my notes [here](https://github.com/coxy1989/mlsabbatical/blob/master/notes/01-karl-rosaen-learning-log-notes.md)) and [resources](http://karlrosaen.com/ml/)
- Scanning resources
- Writing up curriculum page and this log

I hope to get a first draft out today and solicit some feedback. I'm getting bored of planning and hope to start attacking some content later today / tomorrow.

### Sunday, 15/07/2018

A day off at Hampstead Heath. Felt out the Talking Machines Podcast, listening to the first 3 episodes.

In the first episode we were privy to a chat with: Yan LeCun, Yoshua Benugo and Geoff Hinton. I'm aware of LeCun having downloaded the MNIST dataset from [his website](http://yann.lecun.com/exdb/mnist/) a while ago to do [clj_mnist](https://github.com/coxy1989/clj_mnist). I was also aware of Benugo as a co-author of the [deep learning bible](https://www.amazon.co.uk/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618) and I've seen Hinton before in an interview with Andrew Ng in a coursera course - these guys are the Deep Learning Mafia. We also met Kevin Murphy who's a head honcho at google and the author of [Machine Learning a Probabalistic Perspective](https://www.amazon.co.uk/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020) which is fighting it out with [Pattern Recognition and Machine Learning](https://www.amazon.co.uk/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738) to be the canonical advanced level machine learining reference  - I'll hope to graduate onto these books in the not too distant future (I bought PRML last week for the odd flick through, to gauge how deep the pool is).

In the second episode we met Ilya Sutskever. It was interesting hearing him explain his suprise that gradient decent is a good optimization algorithm in NNs given the high-dimensional non-convex surfaces of the functions they compute. He linked this to the AI winter, saying that in the 80s/90s people had failed to train deep neural networks for other reasons (badly initialized weights in paticular) and incorrectly concluded that optimizing deep NNS was not a tractable problem. In the third episode the host dug out a relevant paper from Yoshua Bengioâ€™s Lab entitled: "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", the paper contains empirical and theoretical evidence that saddle points are more frequently the cause of slow training in large NNs than local minima, the paper also proposes some approaches for tackling this problem.

Summary

- A few people to follow on twitter to keep abreast of the moving frontier
- A couple of earmarked textbooks
- Interesting podcast, nice to get a historical context, nice technical level; not superficial but doesn't require frequest pausing / taking of notes. Will continue to listen.


### Saturday, 14/07/2018

I spun up this website and began feeling out some resources that I may decide to include in the curriculum.

