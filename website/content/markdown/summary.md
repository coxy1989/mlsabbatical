<p>
  <a style="padding-right:20px;" href="./index.html">Home</a>
  <a style="padding-right:20px;" href="./curriculum.html">Curriculum</a>
  <a style="padding-right:20px;" href="./learning_log.html">Learning Log</a>
  <a style="padding-right:20px;" href="./bookshelf.html">Bookshelf</a>
</p>

** **Under Construction** **

# Units

### Specialist

- *Bayesian Inferance*

- *Reinforcement Learning*

- *Deep Learning*

- *Kaggling*

### Foundational

- *Introduction to Machine Learning*
	- Course: [Machine Learning by Andrew Ng](https://www.coursera.org/learn/machine-learning)

- *Mathematics for Machine Learning*

	- Course: [Mathematics for Machine Learning](https://www.coursera.org/specializations/mathematics-machine-learning)
	- Textbook: [Mathematical Methods in the Physical Sciences](https://www.amazon.co.uk/Mathematical-Methods-Physical-Sciences-Mary/dp/0471365807) 

- *Supervised Learning*

	- Course: [Statistical Learning]() by Trevor Hastie and Rob Tibshirani
	- Textbooks:
		- [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)
	    - [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)



## Unit 3: Supervised Learning

### Statistical Learning

[The curse of dimensionality](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/statistical_learning/ch2_statistical_learning/curse_of_dimensionality.ipynb)

[Exercises](http://localhost:8888/notebooks/Desktop/mlsabbatical/notebooks/statistical_learning/ch2_statistical_learning/exercises.ipynb)

### Linear Regression

### Classification

### Resampling Methods

### Linear Model Selection and Regularisation

### Moving Beyond Linearity

### Tree Based Methods

### Support Vector Machines

### Unsupervised Learning


## Unit 2: Mathematics for Machine Learning

- *I completed all of the assignments and received [accreditation](https://www.coursera.org/account/accomplishments/specialization/ARMLMTNPZJTD)*

### Linear Algebra

#### Content

- Dot product
- Scalar and vector projection
- Changing basis
- Linear Independence
- Matrix transformations and their composition
- Gaussian elimination
- Matrix inverses
- Determinants
- Einstein summation notation
- Transformations in a changed basis
- Properties of orthogonal matrices
- The Gram-Schmidt process
- Eigenvectors and Eigenvalues
- Power Iteration
- Matrix Diagonalisation

#### Assignments

- [Jupyter assignment: Transformation and change of basis](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/la_ReflectingBear.ipynb)
- [Jupyter assignment: Singular matrices](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/la_IdentifyingSpecialMatrices.ipynb)
- [Jupyter assignment: The Gram-Schmidt process](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/la_GramSchmidtProcess.ipynb)
- [Jupyter assignment: Power iteration and PageRank](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/la_PageRank.ipynb)

The course also multiple choice assignments which, unfortunately it is not possible to link to.

#### Other Resources

- A while ago I did a systematic transit through the [Khan Academy Linear Algebra Track](https://www.khanacademy.org/math/linear-algebra), though verbose in places it was thorough and delivered in Sal's inimitable style.
- Pavel Grinfeld is a superb lecturer, he has a free Linear Algebra series available at [lem.ma](https://www.lem.ma/home).
- Gilbert Strang is the Linear Algebra don, his MIT Linear Algebra class is available free via [opencourseware](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/). Interestingly he is an adviser on the team at Pavel Grinfeld's lem.ma.

### Multivariate Calculus

- Definition of the derivative
- The sum, power, product and chain rules
- The derivatives of 1/x, e^x and trig functions.
- Partial differentiation
- The Jacobian matrix
- The Hessian matrix
- The multivariate chain rule
- Applying the multivariate chain rule to train a neural network
- Building approximate functions
- Maclaurin series
- Taylor series
- Multivariate Taylor series
- Linearisation
- The Newton-Raphson method
- Gradient Descent
- Constrained Optimization: The method of Lagrange Multipliers
- Regression as a minimisation of errors problem
- Distinguish appropriate models for particular data sets
- Fitting functions to data using gradient descent

#### Assignments

- [Jupypter assignment: Backpropagation](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/calc_Backpropagation.ipynb)
- [Jupyter assignment: Gradient Descent](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/calc_Fitting%2Bthe%2Bdistribution%2Bof%2Bheights%2Bdata.ipynb)

The course included multiple choice assignments which, unfortunately it is not possible to link to.

#### Other Resources

- A while ago I did a systematic transit through the [Khan Academy Differential Calculus track](https://www.khanacademy.org/math/differential-calculus), though verbose in places it was thorough and delivered in Sal's inimitable style.
- [Symbolab](https://www.symbolab.com/) is a great resource for checking your steps for careless mistakes if you're dusting off your calculus, it also has a repository of practice problems.

### PCA

#### Content

- Expected Value, Variance and Covariance
- Effect of a linear transformation of the dataset on the moments of the distibution.
- The axiomatic definition of an inner product.
- General definition of norms angles and orthogonality with respect to the inner product.
- The inner product over a continuous domain; the inner product of a pair of functions as an integral and the inner product of a pair of random variables as their covariance.
- Orthogonal projections
- The axiomatic definition of: Groups, Fields, Vector Spaces, Vector Subspaces and their orthogonal complement.
- The objective of PCA; equivalence of maximising variance of projected data and minimizing projection error.
- Deriving a proof for Principal Components Analysis.
- Practical considerations when performing PCA on a data set.

#### Assignments

- [Jupyter assignment: Variance, covariance and affine transformation of a dataset](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/pca_week1.ipynb)
- [Jupyter assignment: Distance and angles in high dimensions](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/pca_week2.ipynb)
- [Jupyter assignment: Orthogonal Projections](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/pca_week3.ipynb)
- [Juptyter assignment: Implement Principal Component Analysis](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/imperial/pca_week4.ipynb)

#### Other Resources

- I found the derivation of PCA on pages 392-3 of [Murphy's MLPP](https://www.amazon.co.uk/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020) helpful.

## Unit 1: An Introduction to Machine Learning

- *I completed all the assignments. I haven't paid the $70 so I can't link to a certificate. I've linked to my assignment solutions below.*

### Content

- Linear Algebra Review
- Octave / Matlab Review
- Linear Regression
	- Univariate and multivariate linear regression
	- Closed form solution using the normal equation
	- Numerical solution with hand rolled gradient descent
	- Numerical solution with an advanced optimisation algorithm: L-BFGS
	- [Octave Lab sheet: Implement Linear Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex1/ex1.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex1/ex1/ex1.m)
	- Polynomial regression
	- The bias-variance trade off
	- Learning curves
	- Regularisation: controlling variance with an L2 penalty
	- Hyperparameter tuning with cross validation
	- [Octave  Lab sheet: Implement Regularised Linear Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex5/ex5.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex5/ex5)
- Binary Classification: Logistic Regression
	- Sigmoid, Odds and Logit
	- Non-linear decision boundries with polynomial features
	- Regularisation: controlling variance with an L2 penalty
	- [Octave Lab sheet: Implement Logistic Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex2/ex2.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex2/ex2)
- Multiclass Classification: Logistic Regression and Neural Networks
	- Multiclass Logistic Regression
	- Vectorised representation of a neural network
	- Forward propagation and prediction
	- [Octave Lab sheet: Implement One vs. All Logistic Regression](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex3/ex3.pdf)
	- [My Solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex3/ex3)
	- Backpropagation
	- Random initialisation
	- Gradient checking
	- Regularisation
	- [Octave Lab sheet: Implement Backpropagation](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex4/ex4.pdf)
	- [My Solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex4/ex4)
- Support Vector Machines
	- Linear decision boundry
	- Non-linear decision boundry with Gaussian kernel function
	- Spam classification with a *bag of words* model
	- [Octave Lab sheet: Implement an SVM](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex6/ex6.pdf)
	- [My Solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex6/ex6)
- Unsupervised Learning
	- K-Means clustering
	- Principal component analysis
	- [Octave Lab sheet: Implement K-means clustering and PCA](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex7/ex7.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex7/ex7)
- Anomaly Detection and Recommender Systems
	- Estimating parameters for a Gaussian
	- The F1 score
	- Anomaly detection in high dimensions
	- Collaborative filtering
	- [Octave Lab sheet: (a) Implement an anomaly detection algorithm (b) Implement a movie recommendation system](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/andrew_ng/machine-learning-ex8/ex8.pdf)
	- [My solution](https://github.com/coxy1989/mlsabbatical/tree/master/notebooks/andrew_ng/machine-learning-ex8/ex8)

## Dead Ends

#### Candidate for Unit 3: Statistical Learning

[Foundations of Machine Learning](https://bloomberg.github.io/foml/?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=email&utm_source=Artificial_Intelligence_Weekly_81#home) by Bloomberg ML EDU

- Ridge Regression, Gradient Descent and SGD
	- [Concept Check 1: Statistical Learning Theory](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/bloomberg/downloads/bloomberg_lec_check_1.pdf)
	- [Solutions: written](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/bloomberg/bloomberg_lec_check_1.md)
	- [Solutions: code](https://github.com/coxy1989/mlsabbatical/blob/master/notebooks/bloomberg/bloomberg_lec_check_1.py)

Discontinued in favour of [Statistical Learning](https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about) by Trevor Hastie and Rob Tibshirani.